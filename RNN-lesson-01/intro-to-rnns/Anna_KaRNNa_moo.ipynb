{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# also import debugger\n",
    "from IPython.core.debugger import set_trace\n",
    "#set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load anna.txt file and start preprocessing\n",
    "'''\n",
    "# load file\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "\n",
    "# select unique strings(alphabet)\n",
    "vocab = set(text)\n",
    "\n",
    "# assign numbers to each alphabet\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab, 0)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "# vocab_to_int = {}\n",
    "# int_to_vocab = {}\n",
    "# for i, c in enumerate(vocab, 0):\n",
    "#     vocab_to_int[c] = i\n",
    "#     int_to_vocab[i] = c\n",
    "\n",
    "    \n",
    "encoded = np.array( [vocab_to_int[c] for c in text], dtype=np.int32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (10, 50)\n",
      "y.shape:  (10, 50)\n",
      "x\n",
      " [[39  0 44  2 19  9 30 20 27 79]\n",
      " [20 44 76 20 67 29 19 20 56 29]\n",
      " [60 42 67 57 79 79 48 78  9 69]\n",
      " [67 20  6 28 30 42 67 56 20  0]\n",
      " [20 42 19 20 42 69 51 20 69 42]\n",
      " [20 49 19 20 52 44 69 79 29 67]\n",
      " [ 0  9 67 20 12 29 76  9 20 32]\n",
      " [55 20 70 28 19 20 67 29 52 20]\n",
      " [19 20 42 69 67 77 19 57 20 11]\n",
      " [20 69 44 42  6 20 19 29 20  0]]\n",
      "\n",
      "y\n",
      " [[ 0 44  2 19  9 30 20 27 79 79]\n",
      " [44 76 20 67 29 19 20 56 29 42]\n",
      " [42 67 57 79 79 48 78  9 69 51]\n",
      " [20  6 28 30 42 67 56 20  0 42]\n",
      " [42 19 20 42 69 51 20 69 42 30]\n",
      " [49 19 20 52 44 69 79 29 67 21]\n",
      " [ 9 67 20 12 29 76  9 20 32 29]\n",
      " [20 70 28 19 20 67 29 52 20 69]\n",
      " [20 42 69 67 77 19 57 20 11  0]\n",
      " [69 44 42  6 20 19 29 20  0  9]]\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x.shape: ', x.shape)\n",
    "print('y.shape: ', y.shape)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder( tf.int32, [batch_size, num_steps], name='inputs' )\n",
    "    targets = tf.placeholder( tf.int32, [batch_size, num_steps], name='targets' )\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder( tf.float32, name='keep_prob' )\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop_out = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell( [drop_out] * num_layers )\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat( lstm_output, axis=1 )\n",
    "    x = tf.reshape( seq_output, [-1, in_size] )\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable( tf.truncated_normal( (in_size, out_size), stddev=0.1 ) )\n",
    "        softmax_b = tf.Variable( tf.zeros(out_size) )   \n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits=logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_onehot = tf.one_hot( targets, num_classes )\n",
    "    y_reshaped = tf.reshape( y_onehot, logits.get_shape() )\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4208...  11.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3356...  0.3619 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8645...  0.3150 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 4.9162...  0.3072 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 3.8808...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.8111...  0.3078 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.6907...  0.3070 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.5737...  0.3069 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.4688...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4331...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3377...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3442...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3346...  0.3075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3468...  0.3070 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3163...  0.3075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3040...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2754...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3047...  0.3100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2700...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2332...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2455...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2363...  0.3087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2339...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2375...  0.3075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2212...  0.3074 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2300...  0.3074 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2286...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.1940...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2057...  0.3071 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2039...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2295...  0.3075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.1997...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1831...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.1972...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1798...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.1892...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1632...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1697...  0.3079 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1609...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1687...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1575...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1612...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1582...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1587...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1499...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1666...  0.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1578...  0.3073 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1729...  0.3104 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1725...  0.3075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1621...  0.3073 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1525...  0.3073 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1555...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1585...  0.3072 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1396...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1502...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1299...  0.3073 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1425...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1403...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1355...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1456...  0.3087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1512...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1630...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1598...  0.3079 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1194...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1254...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1534...  0.3099 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1457...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1011...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1215...  0.3075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1362...  0.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1243...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1481...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1249...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1325...  0.3096 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1300...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1346...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1282...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1158...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1081...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.0995...  0.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1026...  0.3087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1199...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1138...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1012...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.0797...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.0970...  0.3078 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.0833...  0.3078 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 4.7807...  0.3076 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 11.3716...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 9.5624...  0.3119 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 7.1804...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.6545...  0.3090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.2508...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.2483...  0.3109 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.2113...  0.3087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.1857...  0.3090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.1946...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.1699...  0.3096 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.1766...  0.3095 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.1669...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.1585...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.1473...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.1439...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.1297...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.1358...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.1286...  0.3078 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0934...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.1041...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.1054...  0.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0677...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.0904...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0884...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.0815...  0.3079 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.0683...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.0602...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.0627...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.0534...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.0709...  0.3094 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.0726...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.0367...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.0753...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.0454...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.0338...  0.3097 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.0400...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 3.0296...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 3.0128...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 3.0209...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 3.0317...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 3.0020...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 3.0026...  0.3095 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 3.0128...  0.3116 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 2.9645...  0.3111 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 2.9936...  0.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 2.9661...  0.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.9365...  0.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.9247...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.9406...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.9310...  0.3103 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.9378...  0.3093 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.9123...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.9207...  0.3087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.8819...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.8892...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.8908...  0.3079 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.8839...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.8771...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.8659...  0.3079 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.8866...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.8512...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.8732...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.8781...  0.3102 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.8796...  0.3090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.8480...  0.3095 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.8448...  0.3097 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.8111...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.8065...  0.3097 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.7769...  0.3090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.7917...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.7536...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.7830...  0.3094 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.7594...  0.3131 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.7233...  0.3102 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.7157...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.7356...  0.3090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.7387...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.7599...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.8288...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.7152...  0.3098 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.7631...  0.3097 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.7041...  0.3100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.7406...  0.3097 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.7478...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.7419...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.7457...  0.3091 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.7167...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.6826...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.6823...  0.3084 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.6502...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.6267...  0.3081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.6239...  0.3098 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.6307...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.6281...  0.3086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.6126...  0.3088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.6316...  0.3085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.6427...  0.3087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.6077...  0.3091 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.5878...  0.3083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.5649...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.5833...  0.3082 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.5884...  0.3098 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.5764...  0.3090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.5512...  0.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.5882...  0.3094 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.5575...  0.3087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.5549...  0.3092 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.5419...  0.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.5499...  0.3091 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.5318...  0.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.6235...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.5139...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.5193...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.5273...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.5274...  0.3080 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.5247...  0.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.5304...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.5209...  0.3101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.5318...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.5128...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.4936...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.5126...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.4963...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.5380...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.5135...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.5072...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.4887...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.5310...  0.3101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.4990...  0.3113 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.4708...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.4893...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.5177...  0.3074 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.4903...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.4770...  0.3082 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.4619...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.4818...  0.3100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.4661...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.4724...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.4754...  0.3106 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.4661...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.4894...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.4429...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.4466...  0.3084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.4696...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.4486...  0.3088 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.4620...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.4382...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.4202...  0.3118 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.4367...  0.3086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.4198...  0.3101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.4173...  0.3088 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.4210...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.4063...  0.3085 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.4105...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.4210...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.3755...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.4375...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.4164...  0.3127 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.4032...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.4339...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3982...  0.3086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.4223...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.4034...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3945...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.3909...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.4100...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3968...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.3908...  0.3108 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.3923...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.4120...  0.3083 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.4281...  0.3084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.4284...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.4168...  0.3107 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.3867...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.3806...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.4135...  0.3084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.3880...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.3631...  0.3102 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.3563...  0.3102 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.3838...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3982...  0.3082 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.3876...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.3794...  0.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.3584...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.3700...  0.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.4086...  0.3114 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.3605...  0.3106 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.3818...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.3530...  0.3086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.3498...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.3318...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.3807...  0.3108 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.3431...  0.3111 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.3283...  0.3085 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.3077...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.3439...  0.3108 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.3582...  0.3084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.3433...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.3347...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.3432...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.3295...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.3372...  0.3104 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.3174...  0.3102 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.3142...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.3089...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.3223...  0.3107 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.3273...  0.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.3224...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.3161...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.3042...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.3360...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.3218...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.3040...  0.3083 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.3010...  0.3086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.3095...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.3169...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.3062...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.3398...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.3266...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.2861...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.3131...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.3232...  0.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.2980...  0.3103 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.2801...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.2833...  0.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.2524...  0.3088 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.2968...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.2944...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.3126...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.2940...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.3102...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.2772...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.2659...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.2962...  0.3114 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.2806...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.2594...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.2950...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.3004...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.2788...  0.3106 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.2811...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.2614...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.2487...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.2952...  0.3084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.2931...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.2755...  0.3085 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.2840...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.2773...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.2794...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.2990...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.2578...  0.3106 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.2796...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.2511...  0.3103 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.2592...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.2493...  0.3105 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.2529...  0.3086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.2785...  0.3091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.2663...  0.3088 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.2830...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.2503...  0.3081 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.2348...  0.3078 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.2618...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.2908...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.2578...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.2624...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.2357...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.2435...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.2300...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.2374...  0.3086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.2064...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.2637...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.2439...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.2287...  0.3100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.2294...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.2288...  0.3113 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.2317...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.2343...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.2462...  0.3135 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.2448...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.2220...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.2047...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.2070...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.2234...  0.3101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.2436...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.2365...  0.3103 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.2365...  0.3095 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.2136...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.2052...  0.3101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.2121...  0.3101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.1912...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.1724...  0.3148 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.1871...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.2140...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.2006...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.2257...  0.3090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.2062...  0.3087 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.1767...  0.3086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.1909...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.1731...  0.3088 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.1692...  0.3096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.1840...  0.3094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.1975...  0.3098 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.1634...  0.3093 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.1848...  0.3092 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.1833...  0.3097 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.1700...  0.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.1874...  0.3076 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.1763...  0.3100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.1570...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.2650...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.1461...  0.3085 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.1491...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.1560...  0.3101 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.1621...  0.3104 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.1523...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.1667...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.1643...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.1889...  0.3106 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.1532...  0.3103 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.1483...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.1441...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.1642...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.1993...  0.3107 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.1579...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.1487...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.1496...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.1978...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.1578...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.1482...  0.3087 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.1444...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.1919...  0.3084 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.1513...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.1468...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.1400...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.1164...  0.3085 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.1236...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.1403...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.1689...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.1543...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.1360...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.1205...  0.3086 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.1341...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.1590...  0.3085 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.1218...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.1357...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.1289...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.0853...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 2.1006...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.0974...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.1023...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.1275...  0.3110 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.1048...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.0902...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.1197...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 2.0636...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.1195...  0.3120 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.0975...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.1158...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.1417...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 2.0768...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.1455...  0.3101 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.0976...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.0924...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.0893...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.1174...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.1130...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.0970...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.0792...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.1251...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.1032...  0.3106 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.1352...  0.3112 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.1203...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.1036...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.0777...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.1252...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.1057...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 2.0649...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 2.0632...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.0869...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.1228...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.0948...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.1028...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 2.0630...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 2.0796...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.1146...  0.3123 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 2.0860...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 2.0866...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 2.0523...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 2.0640...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 2.0431...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.0940...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 2.0505...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 2.0545...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 2.0238...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 2.0523...  0.3110 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 2.0708...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 2.0531...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 2.0315...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 2.0711...  0.3086 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 2.0334...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 2.0570...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 2.0209...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 2.0211...  0.3103 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 2.0294...  0.3108 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 2.0505...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 2.0400...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 2.0295...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 2.0306...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 2.0189...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 2.0565...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 2.0521...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 2.0257...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 2.0214...  0.3115 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 2.0322...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 2.0367...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 503...  Training loss: 2.0469...  0.3087 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 2.0505...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 2.0582...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 2.0430...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 2.0331...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 2.0396...  0.3087 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 2.0331...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 2.0227...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 2.0262...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.9894...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 2.0312...  0.3109 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 2.0256...  0.3114 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 2.0262...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 2.0211...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 2.0357...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 2.0030...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 2.0102...  0.3104 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 2.0494...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 2.0159...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.9784...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 2.0345...  0.3086 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 2.0354...  0.3108 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 2.0162...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 2.0245...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.9894...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.9996...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 2.0268...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 2.0198...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 2.0098...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 2.0245...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 2.0284...  0.3107 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 2.0113...  0.3112 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 2.0435...  0.3101 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9953...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 2.0334...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 2.0001...  0.3107 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 2.0017...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 2.0140...  0.3085 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.9883...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 2.0303...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 2.0150...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 2.0391...  0.3084 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 2.0128...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.9835...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9909...  0.3096 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 2.0356...  0.3084 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 2.0112...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 2.0209...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.9964...  0.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.9926...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9994...  0.3099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.9923...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.9705...  0.3087 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 2.0382...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 2.0080...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.9906...  0.3081 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 2.0135...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 2.0008...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.9877...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.9855...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 2.0115...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 2.0323...  0.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.9745...  0.3098 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.9772...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.9597...  0.3086 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.9800...  0.3086 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 2.0085...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.9950...  0.3102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.9938...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.9698...  0.3106 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.9674...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.9908...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.9605...  0.3097 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.9474...  0.3092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.9544...  0.3085 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.9760...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.9752...  0.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.9961...  0.3087 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.9724...  0.3101 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.9637...  0.3105 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.9726...  0.3108 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.9488...  0.3105 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.9615...  0.3083 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.9668...  0.3087 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.9734...  0.3116 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.9313...  0.3109 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.9713...  0.3094 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.9538...  0.3091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.9366...  0.3089 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.9651...  0.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.9668...  0.3088 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.9384...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 2.0487...  0.3082 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.9456...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.9403...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.9397...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.9565...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.9141...  0.3102 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.9475...  0.3106 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.9400...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.9774...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.9402...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.9279...  0.3082 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.9186...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.9460...  0.3101 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.9758...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.9362...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.9223...  0.3082 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.9434...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.9698...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.9441...  0.3101 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.9397...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.9297...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.9750...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.9359...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.9360...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.9365...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.9092...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.9152...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.9542...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.9663...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.9472...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.9311...  0.3102 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.9227...  0.3111 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.9395...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.9598...  0.3111 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.9102...  0.3107 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.9211...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.9228...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.8808...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.8820...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.8907...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8987...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.9376...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8883...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.8859...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.9285...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.8562...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.9185...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.9009...  0.3102 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.9037...  0.3107 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.9459...  0.3111 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.8789...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.9652...  0.3101 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.9028...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.9117...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.8970...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.9226...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.9229...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.9042...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.8915...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.9439...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.9023...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.9383...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.9365...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.9213...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8982...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.9344...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.9210...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.8799...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.8839...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8985...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.9336...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.9108...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.9277...  0.3106 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.8867...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8948...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.9225...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.9037...  0.3115 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.9069...  0.3104 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.8683...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.8833...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.8555...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.9090...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.8531...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.8831...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.8501...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.8761...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.8686...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.8634...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.8413...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8999...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.8534...  0.3101 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.8666...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.8544...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.8484...  0.3086 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.8483...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.8818...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.8688...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.8397...  0.3181 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.8518...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.8275...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.8809...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.8696...  0.3104 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.8558...  0.3085 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.8554...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.8516...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.8646...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.8556...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.8672...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.8844...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.8795...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.8607...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.8485...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.8592...  0.3111 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.8574...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.8479...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.8183...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.8601...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.8469...  0.3087 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.8618...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.8490...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.8680...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.8214...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.8302...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.8781...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.8420...  0.3105 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.8008...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.8639...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.8614...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.8426...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.8359...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.8189...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.8150...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.8616...  0.3105 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.8548...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.8559...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.8538...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.8563...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.8556...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.8790...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.8383...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8708...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.8401...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.8408...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.8528...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.8310...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.8515...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.8561...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.8587...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.8477...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.8328...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.8220...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.8596...  0.3110 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.8459...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.8474...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.8216...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.8193...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.8379...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.8312...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7967...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.8636...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.8590...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.8373...  0.3101 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.8570...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.8391...  0.3103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.8295...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.8189...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.8500...  0.3106 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8869...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.8235...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.8197...  0.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.8036...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.8169...  0.3106 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.8402...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.8229...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.8254...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.8126...  0.3093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.8124...  0.3107 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.8370...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.8059...  0.3090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7840...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7916...  0.3105 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.8186...  0.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.8256...  0.3094 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.8451...  0.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.8118...  0.3086 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.8049...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.8165...  0.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7909...  0.3099 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.8063...  0.3091 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.8163...  0.3089 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.8200...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7852...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.8142...  0.3087 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7897...  0.3100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.7832...  0.3098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.8101...  0.3095 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.8035...  0.3082 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.7858...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8932...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7960...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7876...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.8053...  0.3102 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7852...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.7525...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.8038...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.7901...  0.3091 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.8249...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7939...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7704...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7771...  0.3087 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7983...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.8395...  0.3112 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7980...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.7722...  0.3103 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.8023...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.8307...  0.3106 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.8036...  0.3103 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.8095...  0.3085 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.7815...  0.3091 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.8286...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7837...  0.3103 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.8009...  0.3108 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.8025...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.7569...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.7591...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.8049...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.8224...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.8074...  0.3106 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7948...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.7612...  0.3112 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.8008...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.8119...  0.3104 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.7766...  0.3085 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7785...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.7629...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.7542...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.7404...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.7591...  0.3111 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.7554...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7973...  0.3110 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.7609...  0.3088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.7496...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7843...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.7296...  0.3102 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.7711...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.7638...  0.3091 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.7583...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.8083...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.7404...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.8331...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.7749...  0.3088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.7793...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.7748...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7814...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7974...  0.3104 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.7584...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.7632...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.8066...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.7652...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.8204...  0.3088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.8041...  0.3091 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7888...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.7797...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7918...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7901...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.7427...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.7661...  0.3112 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.7605...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.8048...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.7840...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7888...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.7567...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.7620...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7841...  0.3091 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.7698...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.7650...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.7323...  0.3086 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.7579...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.7125...  0.3103 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.7761...  0.3091 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.7305...  0.3110 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.7554...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.7212...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.7395...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.7361...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.7302...  0.3151 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.7182...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.7641...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.7281...  0.3102 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.7373...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.7284...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.7262...  0.3116 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.7336...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.7530...  0.3115 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.7477...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.7195...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.7243...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6996...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.7558...  0.3088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.7450...  0.3102 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.7334...  0.3088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.7325...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.7332...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.7364...  0.3109 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.7373...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.7439...  0.3107 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.7508...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.7546...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.7356...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.7397...  0.3111 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.7364...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.7276...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.7201...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6975...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.7394...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.7289...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.7293...  0.3104 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.7261...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.7417...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.7068...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6960...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.7572...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.7370...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6926...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.7568...  0.3107 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.7516...  0.3109 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.7240...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.7100...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.7050...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.7020...  0.3112 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.7408...  0.3158 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.7404...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.7288...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.7329...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.7430...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.7398...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.7459...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.7185...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7625...  0.3088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.7290...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.7246...  0.3121 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.7458...  0.3104 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.7151...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.7317...  0.3172 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.7434...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.7582...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.7423...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.7105...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6874...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.7328...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.7390...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.7334...  0.3083 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.7158...  0.3086 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.7208...  0.3109 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.7321...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.7172...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6834...  0.3093 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.7526...  0.3087 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.7493...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.7207...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.7331...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.7245...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.7209...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.7188...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.7262...  0.3113 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7775...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.7112...  0.3095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.7054...  0.3116 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.7053...  0.3104 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.7088...  0.3111 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.7334...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.7294...  0.3109 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.7243...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6986...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.7032...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.7274...  0.3090 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6890...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6922...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.6753...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6993...  0.3089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.7068...  0.3104 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.7221...  0.3100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.7057...  0.3101 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6990...  0.3103 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.7144...  0.3103 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6937...  0.3099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.7058...  0.3097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.7101...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6989...  0.3105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6856...  0.3094 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.7169...  0.3088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6704...  0.3096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6688...  0.3087 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.7000...  0.3098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6949...  0.3092 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6876...  0.3105 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7903...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6973...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6898...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6950...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6778...  0.3092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.6563...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6921...  0.3088 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.6779...  0.3108 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.7148...  0.3145 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6883...  0.3092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.6673...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6664...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6880...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.7294...  0.3092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6887...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6816...  0.3141 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.7071...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.7193...  0.3104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6994...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.7039...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6807...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.7236...  0.3087 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6760...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6939...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6901...  0.3094 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.6508...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.6612...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6999...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.7044...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6941...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6801...  0.3087 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.6596...  0.3109 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6915...  0.3167 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6998...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6724...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6730...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.6562...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.6388...  0.3087 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.6296...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.6511...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.6472...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6958...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.6511...  0.3094 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.6401...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6821...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.6374...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.6641...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.6656...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.6596...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.7193...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.6500...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.7202...  0.3101 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6735...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6752...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.6713...  0.3102 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6772...  0.3107 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6949...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.6531...  0.3104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.6492...  0.3102 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6999...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6706...  0.3094 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.7277...  0.3143 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.7008...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6835...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.6714...  0.3079 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6995...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6836...  0.3101 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.6469...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6712...  0.3102 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.6668...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.7167...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6846...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6997...  0.3112 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.6593...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.6610...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6930...  0.3101 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.6684...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.6658...  0.3104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.6304...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.6642...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.6215...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.6854...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.6335...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.6659...  0.3107 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.6368...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.6516...  0.3094 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.6427...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.6409...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.6222...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6728...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.6303...  0.3112 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.6416...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.6298...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.6303...  0.3085 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.6386...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.6659...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.6533...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.6208...  0.3112 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.6287...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.6095...  0.3104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.6661...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.6452...  0.3104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.6412...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.6371...  0.3110 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.6405...  0.3086 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.6370...  0.3102 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.6517...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.6391...  0.3111 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.6498...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6691...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.6391...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.6441...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.6353...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.6378...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.6281...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.6092...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.6593...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.6427...  0.3104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.6464...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.6386...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.6490...  0.3102 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.6196...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.6139...  0.3094 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.6585...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.6457...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.5994...  0.3107 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.6792...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.6656...  0.3092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.6284...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.6194...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.5991...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.6128...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.6565...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.6536...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.6474...  0.3101 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.6345...  0.3126 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.6681...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.6516...  0.3109 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.6498...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.6325...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.6925...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.6286...  0.3110 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.6438...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.6590...  0.3110 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.6241...  0.3110 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.6536...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.6510...  0.3102 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.6690...  0.3105 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.6565...  0.3101 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.6238...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.6067...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.6382...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.6484...  0.3087 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.6403...  0.3109 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.6364...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.6337...  0.3088 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.6416...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.6337...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.5962...  0.3083 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.6531...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.6638...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.6405...  0.3093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.6510...  0.3088 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.6412...  0.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.6316...  0.3101 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.6246...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.6441...  0.3096 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.6963...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.6195...  0.3087 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.6272...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.6146...  0.3100 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.6131...  0.3089 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.6531...  0.3086 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.6207...  0.3088 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.6443...  0.3103 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.6100...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.6092...  0.3092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.6485...  0.3086 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.6002...  0.3109 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.5905...  0.3107 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.5974...  0.3088 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.6169...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.6178...  0.3086 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.6268...  0.3098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.6263...  0.3094 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.6025...  0.3092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.6415...  0.3092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.6147...  0.3108 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.6244...  0.3101 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.6287...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.6205...  0.3086 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.6135...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.6267...  0.3091 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.5945...  0.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.5985...  0.3095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.6163...  0.3097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.6086...  0.3104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.5996...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.7157...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.6195...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.6094...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.6271...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.5983...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.5723...  0.3089 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.6182...  0.3101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.5909...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.6293...  0.3103 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.6028...  0.3086 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.5788...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.5905...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.6098...  0.3107 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.6442...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.5973...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.5894...  0.3113 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.6167...  0.3106 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.6305...  0.3101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.6294...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.6251...  0.3088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.5957...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.6343...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.6001...  0.3114 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.6117...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.6174...  0.3089 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.5778...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.5762...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.6234...  0.3101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.6244...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.6268...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.6118...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.5764...  0.3113 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.6150...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.6294...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.6054...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.6137...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.5817...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.5617...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.5583...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.5776...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.5799...  0.3089 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.6241...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.5883...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.5697...  0.3110 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.6154...  0.3102 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.5606...  0.3105 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.5910...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.5826...  0.3103 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.5875...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.6434...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.5709...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.6488...  0.3083 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.5965...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.6058...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.5841...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.6146...  0.3088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.6204...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.5760...  0.3081 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.5686...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.6335...  0.3112 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.6005...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.6562...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.6290...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.6158...  0.3102 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.5919...  0.3112 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.6249...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.6084...  0.3104 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.5837...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.5953...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.5888...  0.3106 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.6458...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.6067...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.6197...  0.3110 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.5738...  0.3119 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.5927...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.6120...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.5939...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.5964...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.5503...  0.3106 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.5901...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.5463...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.5924...  0.3084 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.5477...  0.3088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.5774...  0.3086 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.5733...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.5775...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.5569...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.5753...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.5516...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.5903...  0.3088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.5603...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.5720...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.5660...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.5725...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.5711...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.5880...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.5928...  0.3154 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.5593...  0.3106 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.5642...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.5382...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.5846...  0.3089 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.5762...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.5761...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.5731...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.5644...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.5735...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.5735...  0.3101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.5711...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.5818...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.5918...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.5637...  0.3107 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.5839...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.5671...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.5689...  0.3105 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.5493...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.5387...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.5840...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.5810...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.5715...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.5670...  0.3106 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.5769...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.5375...  0.3110 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.5252...  0.3113 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.5889...  0.3108 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.5602...  0.3112 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.5336...  0.3115 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.5917...  0.3101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.5866...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.5558...  0.3087 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.5328...  0.3088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.5315...  0.3088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.5345...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.5854...  0.3102 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.5699...  0.3103 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.5831...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.5728...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.5969...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.5724...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.5786...  0.3082 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.5664...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.6191...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.5707...  0.3105 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.5750...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.6014...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.5587...  0.3102 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.5783...  0.3099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.5851...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.6015...  0.3088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.5919...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.5606...  0.3109 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.5314...  0.3109 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.5641...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.5723...  0.3089 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.5646...  0.3089 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.5672...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.5680...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.5728...  0.3107 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.5632...  0.3087 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.5276...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.5840...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.5830...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.5712...  0.3083 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.5715...  0.3087 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.5672...  0.3087 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.5653...  0.3108 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.5614...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.5923...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.6374...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.5624...  0.3103 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.5737...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.5569...  0.3113 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.5499...  0.3104 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.5939...  0.3102 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.5738...  0.3093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.5874...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.5398...  0.3102 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.5454...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.5896...  0.3086 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.5505...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.5350...  0.3089 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.5361...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.5565...  0.3101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.5509...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.5542...  0.3098 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.5578...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.5465...  0.3092 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.5696...  0.3095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.5468...  0.3094 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.5441...  0.3105 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.5525...  0.3097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.5370...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.5290...  0.3100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.5518...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.5194...  0.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.5230...  0.3091 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.5579...  0.3096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.5319...  0.3105 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.5257...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.6633...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.5602...  0.3114 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.5349...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.5427...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.5223...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.5161...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.5513...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.5219...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.5626...  0.3107 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.5373...  0.3098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.5226...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.5336...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.5406...  0.3106 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.5695...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.5297...  0.3104 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.5233...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.5491...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.5699...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.5501...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.5592...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.5429...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.5581...  0.3087 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.5318...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.5527...  0.3086 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.5377...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.4924...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.5086...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.5488...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.5574...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.5568...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.5348...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.5098...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.5478...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.5374...  0.3087 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.5277...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.5352...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.5140...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.4959...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.4893...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.5119...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.5073...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.5652...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.5098...  0.3084 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.4963...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.5404...  0.3084 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.4857...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.5155...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.5282...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.5195...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.5545...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.5062...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.5817...  0.3098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.5339...  0.3109 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.5284...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.5271...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.5251...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.5432...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.5009...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.5063...  0.3088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.5648...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.5255...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.5815...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.5551...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.5425...  0.3123 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.5296...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.5451...  0.3085 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.5480...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.5010...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.5256...  0.3088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.5166...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.5759...  0.3102 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.5419...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.5610...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.5075...  0.3131 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.5328...  0.3083 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.5543...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.5223...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.5158...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.4863...  0.3108 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.5466...  0.3098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.4815...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.5351...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.4900...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.5247...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.5083...  0.3087 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.5135...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.5007...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.5124...  0.3102 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.4837...  0.3098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.5215...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.4887...  0.3115 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.5085...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.4966...  0.3113 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.4996...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.5117...  0.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.5441...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.5238...  0.3106 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.4860...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.4977...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.4735...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.5224...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.5084...  0.3113 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.5075...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.5072...  0.3105 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.5036...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.5036...  0.3107 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.5128...  0.3103 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.5060...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.5068...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.5240...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.5106...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.4994...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.5058...  0.3088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.5018...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.4902...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.4791...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.5117...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.5142...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.5000...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.4953...  0.3098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.4988...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.4701...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.4623...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.5144...  0.3086 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.4912...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.4572...  0.3082 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.5283...  0.3087 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.5153...  0.3088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.4846...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.4704...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.4680...  0.3112 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.4742...  0.3115 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.5242...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.5111...  0.3089 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.5151...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.4994...  0.3086 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.5223...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.5135...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.5079...  0.3088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.4942...  0.3087 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.5504...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.5057...  0.3106 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.5040...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.5343...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.4856...  0.3102 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.5239...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.5183...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.5435...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.5130...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.4905...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.4600...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.4934...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.4948...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.4985...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.4906...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.4950...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.5060...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.5009...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.4554...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.5167...  0.3085 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.5298...  0.3110 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.5051...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.5115...  0.3102 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.5013...  0.3086 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.5007...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.5070...  0.3094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.5158...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.5659...  0.3087 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.5105...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.4886...  0.3098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.4874...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.4821...  0.3088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.5319...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.5001...  0.3104 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.5177...  0.3083 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.4714...  0.3098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.4787...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.5285...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.4849...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.4580...  0.3103 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.4566...  0.3101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.4896...  0.3112 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.5004...  0.3107 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.4885...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.4927...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.4727...  0.3095 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.5144...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.4860...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.4904...  0.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.5048...  0.3096 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.4706...  0.3093 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.4681...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.4902...  0.3099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.4683...  0.3097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.4482...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.4897...  0.3090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.4783...  0.3091 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.4686...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.6118...  0.3091 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.4921...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.4779...  0.3085 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.4921...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.4591...  0.3091 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.4532...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.4757...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.4698...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.4915...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.4856...  0.3090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.4593...  0.3085 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.4695...  0.3102 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.4818...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.5248...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.4791...  0.3106 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.4620...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.4970...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.5106...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.4897...  0.3123 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.5097...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.4812...  0.3082 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.5056...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.4742...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.4902...  0.3104 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.4909...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.4453...  0.3103 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.4449...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.4969...  0.3105 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.4922...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.4991...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.4732...  0.3084 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.4457...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.4924...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.4901...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.4618...  0.3106 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.4809...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.4619...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.4379...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.4272...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.4610...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.4603...  0.3106 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.5017...  0.3090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.4509...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.4466...  0.3087 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.4834...  0.3105 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.4461...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.4633...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.4572...  0.3122 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.4602...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.4890...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.4449...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.5204...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.4811...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.4825...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.4716...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.4728...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.4899...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.4510...  0.3095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.4514...  0.3087 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.5137...  0.3085 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.4765...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.5141...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.4937...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.4933...  0.3104 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.4671...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.4980...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.4943...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.4467...  0.3095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.4735...  0.3105 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.4647...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.5220...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.4913...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.5056...  0.3110 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.4483...  0.3107 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.4661...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.4954...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.4686...  0.3108 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.4584...  0.3102 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.4299...  0.3108 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.4753...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.4313...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.4710...  0.3109 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.4292...  0.3106 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.4648...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.4412...  0.3090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.4679...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.4453...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.4525...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.4322...  0.3091 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.4826...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.4415...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.4498...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.4473...  0.3095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.4484...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.4435...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.4830...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.4741...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.4285...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.4489...  0.3095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.4193...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.4603...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.4526...  0.3091 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.4535...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.4549...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.4520...  0.3090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.4602...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.4678...  0.3095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.4586...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.4604...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.4824...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.4408...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.4732...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.4493...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.4552...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.4369...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.4253...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.4634...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.4664...  0.3095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.4521...  0.3103 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.4468...  0.3090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.4550...  0.3106 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.4168...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.4076...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.4730...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.4450...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.4208...  0.3104 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.4707...  0.3107 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.4595...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.4485...  0.3108 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.4277...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.4101...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.4331...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.4728...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.4665...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.4633...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.4611...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.4802...  0.3087 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.4656...  0.3085 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.4695...  0.3105 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.4506...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.5014...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.4547...  0.3105 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.4438...  0.3084 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.4830...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.4401...  0.3087 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.4853...  0.3103 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.4647...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.4963...  0.3100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.4712...  0.3103 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.4409...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.4268...  0.3122 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.4372...  0.3109 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.4565...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.4634...  0.3115 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.4463...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.4468...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.4691...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.4527...  0.3085 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.4200...  0.3085 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.4717...  0.3104 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.4835...  0.3095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.4586...  0.3090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.4685...  0.3083 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.4621...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.4533...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.4524...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.4763...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.5211...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.4591...  0.3086 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.4477...  0.3086 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.4458...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.4349...  0.3098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.4808...  0.3110 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.4523...  0.3101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.4592...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.4247...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.4393...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.4750...  0.3104 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.4180...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.4068...  0.3124 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.4290...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.4357...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.4422...  0.3091 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.4446...  0.3096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.4457...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.4303...  0.3092 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.4706...  0.3094 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.4362...  0.3090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.4534...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.4576...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.4217...  0.3099 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.4297...  0.3108 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.4449...  0.3106 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.4149...  0.3088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.4078...  0.3097 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.4517...  0.3093 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.4404...  0.3089 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.4233...  0.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.5871...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.4753...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.4763...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.4761...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.4131...  0.3114 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.4112...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.4496...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.4352...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.4597...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.4349...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.4273...  0.3107 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.4401...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.4464...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.4753...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.4338...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.4312...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.4545...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.4699...  0.3110 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.4531...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.4748...  0.3101 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.4297...  0.3107 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.4603...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.4368...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.4571...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.4379...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.3967...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.4177...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.4554...  0.3082 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.4468...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.4611...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.4375...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.4172...  0.3101 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.4451...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.4403...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.4328...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.4304...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.4190...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.3990...  0.3089 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.3858...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.4107...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.4146...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.4692...  0.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.4092...  0.3085 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.4021...  0.3089 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.4443...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.4027...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.4284...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.4199...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.4300...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.4613...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.4055...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.4743...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.4401...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.4478...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.4269...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.4354...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.4556...  0.3100 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.4199...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.4192...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.4742...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.4319...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.4905...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.4595...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.4473...  0.3104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.4366...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.4374...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.4517...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.4180...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.4270...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.4154...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.4748...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.4449...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.4766...  0.3104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.4123...  0.3081 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.4271...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.4521...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.4380...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.4230...  0.3089 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.3987...  0.3086 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.4311...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.3861...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.4442...  0.3108 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.4023...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.4251...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.4059...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.4304...  0.3063 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.4029...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.4080...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.3923...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.4318...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.4036...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.4112...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.4060...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.4086...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.4051...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.4364...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.4318...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.3858...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.4024...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.4022...  0.3104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.4204...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.4138...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.4380...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.4139...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.4169...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.4056...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.4267...  0.3100 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.4319...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.4166...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.4444...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.4087...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.4226...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.4144...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.4145...  0.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.3909...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.3771...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.4324...  0.3084 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.4212...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.4172...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.4070...  0.3101 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.4117...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.3886...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.3719...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.4232...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.4089...  0.3100 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.3762...  0.3104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.4415...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.4281...  0.3104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.4022...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.3757...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.3708...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.3895...  0.3089 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.4443...  0.3100 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.4272...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.4304...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.4140...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.4369...  0.3084 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.4361...  0.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.4199...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.4141...  0.3110 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.4762...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.4271...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.4139...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.4519...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.4030...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.4390...  0.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.4359...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.4558...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.4499...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.4101...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.3850...  0.3101 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.4004...  0.3104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.4276...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.4148...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.4115...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.4164...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.4189...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.4091...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.3784...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.4335...  0.3125 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.4421...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.4263...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.4247...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.4216...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.4087...  0.3100 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.4132...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.4301...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.4887...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.4203...  0.3106 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.4118...  0.3128 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.4018...  0.3104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.3961...  0.3105 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.4450...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.4142...  0.3102 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.4249...  0.3098 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.3932...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.4006...  0.3106 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.4472...  0.3092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.3967...  0.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.3823...  0.3103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.3944...  0.3099 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.3974...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.4020...  0.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.4068...  0.3085 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.4057...  0.3087 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.3983...  0.3091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.4365...  0.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.4027...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.4052...  0.3089 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.4169...  0.3100 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.3858...  0.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.3976...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.4161...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.3904...  0.3093 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.3836...  0.3096 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.4144...  0.3090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.3999...  0.3097 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.3966...  0.3111 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.5346...  0.3125 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.4198...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.3975...  0.3099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.4327...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.3880...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.3743...  0.3099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.4033...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.3953...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.4168...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.4025...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.3884...  0.3107 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.3939...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.4123...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.4291...  0.3104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.3957...  0.3103 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.3881...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.4101...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.4282...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.4203...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.4340...  0.3096 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.3988...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.4112...  0.3096 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.3906...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.4190...  0.3107 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.4145...  0.3105 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.3602...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.3773...  0.3138 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.4114...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.4174...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.4215...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.3931...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.3724...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.4128...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.4033...  0.3084 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.3972...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.4021...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.3768...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.3593...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.3473...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.3806...  0.3086 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.3748...  0.3103 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.4352...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.3879...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.3842...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.4121...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.3708...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.3919...  0.3084 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.3884...  0.3099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.3992...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.4126...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.3818...  0.3101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.4449...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.4030...  0.3107 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.4179...  0.3111 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.4021...  0.3104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.4012...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.4250...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.3953...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.3824...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.4515...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.4111...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.4530...  0.3106 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.4243...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.4048...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.3941...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.4101...  0.3101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.4093...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.3837...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.4023...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3836...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.4450...  0.3088 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.4211...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.4364...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.3857...  0.3096 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.3948...  0.3101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.4201...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.3993...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.3866...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.3446...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.4162...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.3550...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.3946...  0.3096 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.3556...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.3884...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.3711...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.3819...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.3764...  0.3106 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.3708...  0.3103 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.3603...  0.3089 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.4040...  0.3110 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.3784...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.3778...  0.3193 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.3797...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.3710...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.3773...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.4124...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.4011...  0.3101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.3674...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.3711...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.3535...  0.3110 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.4011...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.3848...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.3998...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.3828...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.3828...  0.3096 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.3754...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.4003...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.3822...  0.3088 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.3827...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.4046...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.3719...  0.3111 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.4000...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.3915...  0.3088 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.3842...  0.3104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.3650...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.3480...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.3979...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.3990...  0.3088 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.3842...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.3821...  0.3080 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.3856...  0.3109 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.3487...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.3395...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.3997...  0.3104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.3742...  0.3113 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.3523...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.4012...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.3955...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.3809...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.3494...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.3455...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.3602...  0.3096 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.4041...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.3977...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.4056...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.3908...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.4212...  0.3111 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.3970...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.3922...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.3935...  0.3099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.4361...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.3933...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.3829...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.4216...  0.3099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.3707...  0.3104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.4204...  0.3107 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.4047...  0.3101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.4334...  0.3088 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.4060...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.3805...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.3599...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.3712...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.3912...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.3862...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.3677...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.3811...  0.3104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.3932...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.3852...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.3435...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.4022...  0.3103 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.4062...  0.3086 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.3843...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.3861...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.3887...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.3843...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.3876...  0.3088 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.4030...  0.3091 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.4598...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.3935...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3803...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.3766...  0.3097 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.3658...  0.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.4090...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.3788...  0.3099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.3995...  0.3085 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.3521...  0.3104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.3814...  0.3101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.4194...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.3615...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.3526...  0.3094 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.3566...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.3763...  0.3093 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.3882...  0.3099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.3678...  0.3107 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.3833...  0.3100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.3687...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.4024...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.3716...  0.3103 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.3813...  0.3108 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.3799...  0.3098 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.3578...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.3690...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.3814...  0.3090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.3495...  0.3101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.3494...  0.3095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.3851...  0.3103 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.3662...  0.3102 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.3618...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.5081...  0.3104 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.3895...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.3695...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.3945...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.3557...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.3424...  0.3101 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.3796...  0.3122 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.3549...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.3831...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.3732...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.3624...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.3778...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.3782...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.3908...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.3680...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.3537...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.3914...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.4015...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.3842...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.3989...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.3691...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3995...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.3684...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.3904...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.3784...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.3381...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.3463...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.3853...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.3835...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.3925...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.3572...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.3426...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.3860...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.3806...  0.3087 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.3721...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.3793...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.3509...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.3431...  0.3110 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.3249...  0.3110 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.3522...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.3542...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.4185...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.3574...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.3427...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.3862...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.3464...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.3502...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.3681...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.3759...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.3947...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.3490...  0.3114 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.4135...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.3825...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.3723...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.3705...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.3670...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.3906...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.3516...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.3542...  0.3104 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.4100...  0.3087 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.3851...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.4249...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.3921...  0.3081 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.3902...  0.3109 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.3643...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.3860...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.3990...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.3589...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.3766...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.3545...  0.3101 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.4168...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3941...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.4060...  0.3101 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.3551...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.3690...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.3869...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.3792...  0.3101 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.3596...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.3290...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.3755...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.3367...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.3669...  0.3159 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.3349...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.3647...  0.3105 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.3531...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.3681...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.3499...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.3464...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.3325...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.3788...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.3481...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.3526...  0.3154 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.3500...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.3384...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.3449...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.3825...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.3720...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.3300...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.3384...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.3379...  0.3106 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.3701...  0.3113 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.3559...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.3716...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.3521...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.3624...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.3557...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.3711...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.3630...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.3548...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.3827...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.3414...  0.3104 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.3663...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.3645...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.3516...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.3360...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.3244...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.3732...  0.3086 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.3682...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.3480...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.3570...  0.3086 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.3594...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.3252...  0.3105 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.3146...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.3610...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.3451...  0.3107 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.3173...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.3739...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.3709...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.3430...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.3133...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.3194...  0.3101 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.3389...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.3757...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.3625...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.3685...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.3663...  0.3106 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.3941...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.3769...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.3702...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.3602...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.4131...  0.3087 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.3655...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.3485...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.4043...  0.3104 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.3403...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.3778...  0.3092 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.3749...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.4042...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.3850...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.3546...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.3232...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.3349...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.3723...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.3570...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.3515...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.3597...  0.3072 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.3607...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.3547...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.3214...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.3763...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.3914...  0.3106 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.3627...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.3615...  0.3103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.3554...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.3658...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.3580...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.3904...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.4236...  0.3095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.3689...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.3614...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.3447...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.3466...  0.3108 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.3945...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.3570...  0.3082 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.3745...  0.3100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.3391...  0.3091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.3433...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.3950...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.3442...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.3212...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.3394...  0.3105 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.3614...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.3532...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.3478...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.3517...  0.3102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.3492...  0.3087 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.3882...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.3548...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.3499...  0.3094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.3532...  0.3098 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.3347...  0.3093 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.3392...  0.3089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.3657...  0.3096 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.3391...  0.3088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.3219...  0.3090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.3600...  0.3099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.3507...  0.3097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.3370...  0.3100 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.4783...  0.3089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.3786...  0.3104 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.3553...  0.3106 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.3748...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.3341...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.3181...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.3640...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.3396...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.3642...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.3507...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.3351...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.3512...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.3570...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.3605...  0.3095 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.3462...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.3255...  0.3090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.3598...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.3735...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.3503...  0.3099 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.3786...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.3458...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.3672...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.3385...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.3689...  0.3099 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.3539...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.3055...  0.3090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.3213...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.3696...  0.3095 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.3660...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.3639...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.3335...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.3165...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.3500...  0.3095 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.3591...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.3557...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.3557...  0.3084 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.3314...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.3144...  0.3108 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.3023...  0.3109 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.3358...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.3308...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.3901...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.3397...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.3184...  0.3100 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.3522...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.3234...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.3386...  0.3100 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.3352...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.3481...  0.3090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.3682...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.3264...  0.3100 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.3916...  0.3101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.3538...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.3556...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.3522...  0.3099 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.3476...  0.3112 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.3644...  0.3108 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.3319...  0.3107 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.3235...  0.3101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.3811...  0.3087 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.3688...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.4053...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.3749...  0.3105 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.3561...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.3522...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.3606...  0.3104 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.3611...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.3338...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.3571...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.3394...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.3940...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.3708...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.3742...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.3270...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.3426...  0.3086 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.3659...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.3524...  0.3086 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.3357...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.3044...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.3593...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.3061...  0.3107 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.3505...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.3160...  0.3089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.3460...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.3216...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.3446...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.3220...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.3132...  0.3137 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.3121...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.3589...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.3271...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.3321...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.3225...  0.3089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.3195...  0.3083 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.3326...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.3647...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.3393...  0.3085 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.3082...  0.3126 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.3208...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.3005...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.3393...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.3332...  0.3090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.3489...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.3362...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.3365...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.3315...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.3477...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.3372...  0.3095 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.3342...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.3564...  0.3107 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.3216...  0.3083 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.3426...  0.3087 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.3301...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.3333...  0.3089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.3187...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.3080...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.3437...  0.3152 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.3483...  0.3110 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.3289...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.3302...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.3429...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.2972...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.2986...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.3430...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.3240...  0.3105 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.3081...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.3478...  0.3110 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.3470...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.3289...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.2997...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.2910...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.3120...  0.3099 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.3613...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.3436...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.3553...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.3433...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.3751...  0.3110 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.3470...  0.3101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.3490...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.3256...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.3891...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.3487...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.3367...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.3705...  0.3083 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.3263...  0.3089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.3531...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.3475...  0.3103 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.3756...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.3627...  0.3095 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.3388...  0.3097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.3036...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.3103...  0.3100 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.3527...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.3430...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.3275...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.3259...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.3385...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.3293...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.3096...  0.3090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.3562...  0.3099 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.3577...  0.3109 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.3473...  0.3101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.3385...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.3398...  0.3089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.3401...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.3333...  0.3090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.3627...  0.3089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.4033...  0.3101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.3493...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.3394...  0.3101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.3304...  0.3100 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.3334...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.3609...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.3365...  0.3104 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.3384...  0.3107 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.3112...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.3241...  0.3104 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.3685...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.3185...  0.3102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.3056...  0.3096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.3194...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.3267...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.3261...  0.3088 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.3325...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.3285...  0.3093 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.3208...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.3574...  0.3094 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.3234...  0.3105 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.3309...  0.3091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.3404...  0.3087 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.3027...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.3186...  0.3087 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.3379...  0.3092 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.3106...  0.3105 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.2872...  0.3105 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.3375...  0.3086 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.3260...  0.3098 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.3298...  0.3109 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.4612...  0.3100 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.3585...  0.3111 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.3349...  0.3129 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.3484...  0.3116 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.3112...  0.3103 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.2975...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.3350...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.3190...  0.3107 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.3335...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.3199...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.3151...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.3252...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.3437...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.3402...  0.3107 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.3238...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.3090...  0.3104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.3360...  0.3105 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.3562...  0.3085 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.3393...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.3566...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.3276...  0.3087 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.3469...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.3304...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.3474...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.3348...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.3012...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.3053...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.3426...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.3378...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.3393...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.3194...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.3083...  0.3087 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.3403...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.3338...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.3194...  0.3103 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.3259...  0.3106 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.3059...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.2921...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.2818...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.3031...  0.3103 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.2994...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.3661...  0.3100 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.3184...  0.3117 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.2977...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.3397...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.3006...  0.3087 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.3147...  0.3110 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.3210...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.3313...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.3553...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.3001...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.3590...  0.3103 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.3283...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.3348...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.3248...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.3224...  0.3085 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.3454...  0.3099 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.3141...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.3049...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.3583...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.3327...  0.3083 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.3844...  0.3100 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.3492...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.3320...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.3297...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.3348...  0.3106 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.3415...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.3209...  0.3113 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.3287...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.3113...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.3752...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.3477...  0.3111 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.3584...  0.3111 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.3063...  0.3086 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.3270...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.3467...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.3263...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.3049...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.2778...  0.3106 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.3323...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.2890...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.3224...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.2994...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.3127...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.2948...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.3290...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.3017...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.3030...  0.3082 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.2840...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.3351...  0.3099 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.2955...  0.3104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.3049...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.2977...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.3016...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.3069...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.3419...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.3209...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.2821...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.3008...  0.3108 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.2865...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.3211...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.3077...  0.3087 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.3232...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.3160...  0.3104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.3140...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.3086...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.3276...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.3170...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.3063...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.3371...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.3096...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.3292...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.3174...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.3032...  0.3107 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.2849...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.2806...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.3377...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.3181...  0.3106 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.3109...  0.3099 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.3091...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.3066...  0.3099 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.2861...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.2749...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.3207...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.3118...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.2767...  0.3110 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.3296...  0.3084 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.3230...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.3015...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.2787...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.2812...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.2884...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.3327...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.3203...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.3335...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.3221...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.3472...  0.3106 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.3292...  0.3086 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.3207...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.3260...  0.3103 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.3677...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.3339...  0.3086 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.3153...  0.3090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.3461...  0.3106 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.2963...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.3370...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.3273...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.3536...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.3481...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.3163...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.2942...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.2949...  0.3103 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.3214...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.3193...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.3007...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.3079...  0.3113 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.3180...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.3133...  0.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.2817...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.3340...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.3362...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.3246...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.3245...  0.3097 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.3127...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.3236...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.3177...  0.3105 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.3536...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.3873...  0.3104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.3266...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.3225...  0.3092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.3103...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.3079...  0.3084 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.3450...  0.3107 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.3255...  0.3111 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.3291...  0.3129 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2939...  0.3091 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.3096...  0.3087 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.3511...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.3019...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.3013...  0.3096 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.3028...  0.3100 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.3109...  0.3106 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.3127...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.3006...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.3079...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.3001...  0.3100 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.3393...  0.3107 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.3066...  0.3098 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.3126...  0.3104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.3178...  0.3094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.2949...  0.3101 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.3006...  0.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.3141...  0.3085 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.2936...  0.3088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.2854...  0.3089 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.3251...  0.3099 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.2960...  0.3093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.2954...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.4421...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.3291...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.3132...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.3320...  0.3105 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.2841...  0.3110 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.2816...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.3203...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.3025...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.3244...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.3021...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.3027...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.3080...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.3193...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.3332...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.3022...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.2952...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.3240...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.3325...  0.3086 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.3203...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.3362...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.3021...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.3199...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.3049...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.3338...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.3108...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.2656...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.2766...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.3273...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.3172...  0.3084 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.3213...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.2952...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.2851...  0.3090 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.3243...  0.3077 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.3070...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.3072...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.3086...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.2851...  0.3084 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.2614...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.2741...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.2862...  0.3094 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.2800...  0.3104 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.3534...  0.3106 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.2909...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.2842...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.3146...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.2779...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.2991...  0.3086 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.3019...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.3065...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.3204...  0.3115 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.2881...  0.3106 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.3346...  0.3095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.3063...  0.3086 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.3261...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.2977...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.3144...  0.3105 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.3180...  0.3105 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.2825...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.2829...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.3461...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.3154...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.3526...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.3314...  0.3086 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.3154...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.3054...  0.3084 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.3217...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.3314...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.2960...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.3156...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.2981...  0.3104 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.3527...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.3221...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.3425...  0.3095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.2912...  0.3106 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.3090...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.3236...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.3130...  0.3094 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2924...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.2678...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.3099...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.2634...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.3117...  0.3086 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.2804...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.2974...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.2797...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.3082...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.2768...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.2788...  0.3095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.2617...  0.3084 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.3048...  0.3111 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.2886...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.2981...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.2836...  0.3085 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.2762...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.2940...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.3197...  0.3107 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.3118...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.2680...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.2802...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.2785...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.3035...  0.3104 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.2899...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.3079...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.2938...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.2963...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.2864...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.2989...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.2992...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.2795...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.3131...  0.3142 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.2893...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.3069...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.2945...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.2891...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.2770...  0.3094 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.2596...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.3096...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.2947...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.2929...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.2853...  0.3128 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.3026...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.2600...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.2597...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.2952...  0.3090 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.2926...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.2608...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.3154...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.3037...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.2850...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.2555...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.2583...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.2818...  0.3090 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.3299...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.3015...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.3094...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.3056...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.3405...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.3198...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.3022...  0.3108 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.3052...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.3550...  0.3094 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.3116...  0.3106 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.3020...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.3271...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.2868...  0.3094 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.3263...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.3020...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.3301...  0.3120 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.3238...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.3025...  0.3095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.2675...  0.3106 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.2759...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.3147...  0.3090 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.2981...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.2908...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.2940...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.3064...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.2906...  0.3095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.2753...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.3178...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.3286...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.3096...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.3017...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.2991...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.2973...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.2885...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.3323...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.3716...  0.3095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.3124...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.3021...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.2908...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.2915...  0.3094 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.3234...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.3075...  0.3105 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.3102...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.2716...  0.3088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.2964...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.3424...  0.3118 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.2822...  0.3092 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.2805...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.2869...  0.3100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.2977...  0.3102 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.2952...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.2906...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.2979...  0.3087 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.2777...  0.3089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.3247...  0.3095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.2808...  0.3098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.2876...  0.3105 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.3022...  0.3097 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.2729...  0.3109 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.2829...  0.3113 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.3050...  0.3099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.2810...  0.3101 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.2538...  0.3093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.3063...  0.3091 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.2912...  0.3096 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.2925...  0.3083 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.4320...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.3195...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.3002...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.3140...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.2708...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.2588...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.2998...  0.3104 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.2795...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.2977...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.2967...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.2850...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.2952...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.2997...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.3044...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.2840...  0.3186 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.2749...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.2991...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.3216...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.3010...  0.3104 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.3287...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.2894...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.3035...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.2883...  0.3084 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.3134...  0.3088 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.2957...  0.3104 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.2453...  0.3100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.2692...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.3031...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.3095...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.3040...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.2674...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.2651...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.3042...  0.3103 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.2898...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.2742...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.2923...  0.3108 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.2663...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.2546...  0.3104 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.2505...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.2773...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.2706...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.3241...  0.3103 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.2809...  0.3103 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.2614...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.3051...  0.3104 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.2628...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.2789...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.2907...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.3005...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.3064...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.2654...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.3351...  0.3120 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.2894...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.2931...  0.3086 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.2816...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.2968...  0.3088 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.3100...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.2894...  0.3110 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.2716...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.3207...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.2983...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.3422...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.3194...  0.3113 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.3030...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.2833...  0.3116 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.3059...  0.3099 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.3136...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.2652...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.2955...  0.3099 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.2868...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.3439...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.3060...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.3241...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.2660...  0.3100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.2832...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.3116...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.2896...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.2771...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.2445...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2902...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.2531...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.2914...  0.3088 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.2574...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.2833...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.2596...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.2908...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.2633...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.2692...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.2512...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.2967...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.2595...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.2722...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.2623...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.2646...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.2770...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.3010...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.2916...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.2512...  0.3099 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.2654...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.2609...  0.3088 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.2827...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.2785...  0.3100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.2979...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.2813...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.2778...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.2824...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.2934...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.2839...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.2607...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.3099...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.2642...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.2882...  0.3100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.2879...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.2704...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.2681...  0.3103 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.2497...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.2917...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.3007...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.2740...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.2772...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.2926...  0.3099 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.2509...  0.3085 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.2495...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.2780...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.2681...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.2424...  0.3099 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.2917...  0.3088 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.2868...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.2597...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.2467...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.2322...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.2636...  0.3088 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.3100...  0.3104 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.2980...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.2878...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.2826...  0.3103 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.3194...  0.3109 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.2911...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.2910...  0.3110 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.2853...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.3478...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.2834...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.2807...  0.3107 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.3243...  0.3099 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.2757...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.3095...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.2916...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.3124...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.3049...  0.3085 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.2773...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.2538...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.2645...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.2879...  0.3089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.2771...  0.3099 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.2700...  0.3105 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.2779...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.2834...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.2747...  0.3102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.2528...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.3055...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.2971...  0.3101 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.2855...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.2742...  0.3086 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.2697...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.2740...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.2768...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.3051...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.3605...  0.3106 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.2995...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.2843...  0.3093 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.2737...  0.3090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.2768...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.3159...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.2830...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.2904...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.2579...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.2837...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.3264...  0.3097 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.2657...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.2598...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.2707...  0.3091 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.2771...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.2747...  0.3094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.2729...  0.3098 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.2781...  0.3087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.2679...  0.3096 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.3135...  0.3149 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.2761...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.2851...  0.3095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.2862...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.2401...  0.3105 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.2638...  0.3106 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.2837...  0.3111 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.2627...  0.3111 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.2504...  0.3103 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.2852...  0.3082 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.2693...  0.3092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.2766...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.4138...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.3012...  0.3087 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.2843...  0.3085 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.3147...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.2578...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.2452...  0.3086 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.2887...  0.3108 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.2813...  0.3083 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.2931...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.2688...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.2713...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.2698...  0.3105 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.2950...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.2960...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.2764...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.2575...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.2954...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.3169...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.2820...  0.3108 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.3085...  0.3095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.2775...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.2929...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.2708...  0.3106 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.3000...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.2802...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.2388...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.2536...  0.3095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.3005...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.2792...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.2945...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.2720...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.2621...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.2903...  0.3087 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.2856...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.2693...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.2770...  0.3087 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.2461...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.2277...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.2233...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.2628...  0.3096 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.2493...  0.3096 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.3193...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.2671...  0.3106 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.2542...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.2879...  0.3084 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.2546...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.2752...  0.3095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.2645...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.2869...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.2914...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.2468...  0.3102 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.3030...  0.3110 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.2823...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.2931...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.2736...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.2796...  0.3086 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.2872...  0.3105 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.2607...  0.3096 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.2605...  0.3102 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.3061...  0.3096 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.2838...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.3253...  0.3095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.3048...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.2871...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.2765...  0.3102 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.2839...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.2963...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.2590...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.2797...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.2660...  0.3109 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.3212...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.3000...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.3042...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.2539...  0.3123 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.2822...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.2944...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.2744...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.2607...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.2412...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.2836...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.2323...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2755...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.2410...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.2636...  0.3085 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.2609...  0.3095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.2722...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.2549...  0.3110 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.2562...  0.3117 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.2386...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.2858...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.2438...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.2595...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.2506...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.2594...  0.3086 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.2590...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.2876...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.2766...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.2415...  0.3104 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.2552...  0.3095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.2514...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.2716...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.2632...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.2737...  0.3131 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.2642...  0.3106 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.2756...  0.3114 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.2567...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.2777...  0.3086 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.2734...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.2584...  0.3111 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.2897...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.2535...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.2645...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.2633...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.2556...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.2454...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.2286...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.2898...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.2708...  0.3107 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.2625...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.2693...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.2721...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.2332...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.2317...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.2784...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.2613...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.2245...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.2824...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.2780...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.2552...  0.3105 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.2309...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.2243...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.2490...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.2868...  0.3103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.2812...  0.3104 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.2759...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.2687...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.2994...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.2860...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.2703...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.2782...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.3145...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.2781...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.2687...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.3075...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.2525...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.2998...  0.3102 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.2905...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.3012...  0.3107 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.2889...  0.3089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.2655...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.2442...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.2457...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.2803...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.2723...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.2692...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.2540...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.2714...  0.3107 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.2632...  0.3106 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.2384...  0.3096 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.2857...  0.3104 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.2904...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.2761...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.2702...  0.3093 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.2700...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.2687...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.2730...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.2946...  0.3102 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.3305...  0.3101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.2797...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.2650...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.2665...  0.3112 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.2664...  0.3105 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.2909...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.2738...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.2806...  0.3107 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.2389...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.2577...  0.3119 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.3097...  0.3098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.2503...  0.3094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.2388...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.2554...  0.3090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2729...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.2619...  0.3112 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.2599...  0.3095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.2512...  0.3110 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.2512...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.3096...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.2670...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.2634...  0.3091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.2605...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.2317...  0.3100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.2590...  0.3110 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.2737...  0.3092 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.2533...  0.3088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.2230...  0.3085 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.2704...  0.3099 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.2606...  0.3097 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.2631...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.3929...  0.3085 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.2821...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.2655...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.2930...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.2451...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.2328...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.2662...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.2561...  0.3108 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.2824...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.2574...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.2488...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.2525...  0.3087 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.2673...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.2777...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.2563...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.2481...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.2720...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.2942...  0.3104 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.2749...  0.3123 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.2866...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.2741...  0.3106 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.2872...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.2646...  0.3096 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.3007...  0.3106 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.2660...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.2267...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.2380...  0.3103 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.2845...  0.3096 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.2717...  0.3087 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.2838...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.2553...  0.3106 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.2474...  0.3087 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.2714...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2601...  0.3110 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.2516...  0.3107 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.2642...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.2392...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.2214...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.2286...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.2464...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.2357...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.3103...  0.3100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.2469...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.2378...  0.3107 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.2772...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.2387...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.2500...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.2597...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.2679...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.2790...  0.3100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.2369...  0.3105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.2935...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.2612...  0.3116 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.2756...  0.3115 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.2583...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.2602...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.2801...  0.3086 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.2545...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.2446...  0.3109 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.2951...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.2758...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.3097...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.2847...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.2756...  0.3086 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.2693...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.2720...  0.3108 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.2874...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.2466...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.2645...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.2549...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.3109...  0.3107 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.2834...  0.3107 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.2986...  0.3105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.2468...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.2651...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.2843...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.2725...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.2489...  0.3103 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.2240...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.2648...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.2233...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.2554...  0.3100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.2320...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.2538...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.2298...  0.3100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.2587...  0.3104 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.2342...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.2310...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.2337...  0.3084 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.2684...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.2383...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.2454...  0.3085 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.2335...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.2317...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.2541...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.2563...  0.3105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.2614...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.2214...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.2480...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.2294...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.2538...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.2481...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.2638...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.2492...  0.3100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.2513...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.2485...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.2705...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.2547...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.2418...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.2836...  0.3096 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.2412...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.2650...  0.3083 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.2519...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.2476...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.2369...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.2176...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.2602...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.2655...  0.3110 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.2477...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.2418...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.2447...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.2252...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.2140...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.2566...  0.3107 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.2511...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.2178...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.2691...  0.3102 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.2601...  0.3105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.2390...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.2099...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.2109...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.2384...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.2724...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.2628...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.2575...  0.3087 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.2463...  0.3096 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.2971...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.2643...  0.3096 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.2617...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.2587...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.3050...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.2638...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.2526...  0.3112 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.2931...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.2447...  0.3102 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.2815...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.2637...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.2923...  0.3093 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.2838...  0.3105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.2515...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.2291...  0.3102 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.2336...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.2713...  0.3105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.2561...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.2507...  0.3091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.2510...  0.3111 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.2550...  0.3119 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.2480...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.2189...  0.3100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.2808...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.2750...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.2533...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.2563...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.2460...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.2495...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.2572...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.2854...  0.3104 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.3183...  0.3103 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.2692...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.2517...  0.3086 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.2430...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.2515...  0.3092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.2810...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.2641...  0.3085 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.2660...  0.3090 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.2280...  0.3096 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.2526...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.2947...  0.3084 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.2383...  0.3104 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.2274...  0.3108 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.2397...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.2461...  0.3089 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.2435...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.2431...  0.3095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.2486...  0.3097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.2321...  0.3113 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.2853...  0.3112 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.2523...  0.3109 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.2525...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.2546...  0.3113 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.2318...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.2443...  0.3113 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.2637...  0.3099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.2455...  0.3101 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.2175...  0.3094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.2561...  0.3088 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.2375...  0.3098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.2422...  0.3108 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.3851...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.2730...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.2453...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.2746...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.2293...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.2186...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.2519...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.2330...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.2521...  0.3105 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.2464...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.2394...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.2488...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.2674...  0.3109 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.2589...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.2494...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.2284...  0.3115 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.2733...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.2664...  0.3103 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.2501...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.2801...  0.3092 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.2476...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.2732...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.2603...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.2788...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.2598...  0.3103 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.2146...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.2278...  0.3117 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.2739...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.2718...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.2549...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.2273...  0.3085 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.2233...  0.3088 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.2520...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.2533...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.2406...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.2515...  0.3088 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.2197...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.2081...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.2053...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.2390...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.2257...  0.3106 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.2891...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.2413...  0.3092 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.2194...  0.3089 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.2476...  0.3087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.2270...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.2382...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.2431...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.2452...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.2649...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.2145...  0.3105 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.2915...  0.3119 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.2407...  0.3117 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.2518...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.2469...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.2541...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.2589...  0.3106 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.2404...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.2318...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.2886...  0.3116 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.2721...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.2897...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.2763...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.2614...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.2512...  0.3092 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.2520...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.2828...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.2394...  0.3104 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.2610...  0.3119 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.2407...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.2928...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.2723...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.2805...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.2276...  0.3087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.2469...  0.3090 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.2611...  0.3084 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.2537...  0.3104 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.2401...  0.3090 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.2071...  0.3088 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.2495...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.2079...  0.3107 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.2529...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.2208...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.2406...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.2229...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.2521...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.2282...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.2311...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.2154...  0.3105 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.2505...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.2274...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.2363...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.2282...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.2222...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.2433...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.2612...  0.3089 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.2538...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.2145...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.2273...  0.3088 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.2107...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.2529...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.2374...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.2554...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.2334...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.2305...  0.3103 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.2356...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.2374...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.2375...  0.3089 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.2396...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.2668...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.2335...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.2509...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.2409...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.2254...  0.3090 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.2229...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.2054...  0.3107 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.2505...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.2460...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.2394...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.2473...  0.3106 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.2426...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.2169...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.2141...  0.3089 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.2397...  0.3085 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.2368...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.2009...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.2460...  0.3092 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.2496...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.2262...  0.3110 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.2023...  0.3105 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.1982...  0.3088 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.2356...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.2737...  0.3110 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.2476...  0.3119 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.2484...  0.3092 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.2451...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.2768...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.2594...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.2501...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.2490...  0.3086 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.2935...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.2561...  0.3089 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.2399...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.2734...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.2233...  0.3087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.2554...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.2538...  0.3110 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.2761...  0.3116 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.2840...  0.3087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.2490...  0.3087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.2187...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.2192...  0.3087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.2591...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.2467...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.2313...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.2464...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.2496...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.2288...  0.3120 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.2235...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.2537...  0.3088 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.2551...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.2505...  0.3105 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.2415...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.2383...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.2423...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.2390...  0.3086 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.2661...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.2960...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.2557...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.2421...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.2343...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.2416...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.2751...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.2566...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.2591...  0.3088 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.2153...  0.3087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.2466...  0.3090 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.2864...  0.3101 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.2257...  0.3102 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.2209...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.2297...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.2468...  0.3098 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.2422...  0.3105 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.2312...  0.3095 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.2365...  0.3096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.2169...  0.3097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.2650...  0.3094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.2412...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.2399...  0.3110 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.2328...  0.3099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.2111...  0.3107 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.2303...  0.3104 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.2521...  0.3093 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.2291...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.2090...  0.3091 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.2414...  0.3100 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.2322...  0.3092 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.2338...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.3630...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.2542...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.2392...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.2631...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.2251...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.2096...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.2368...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.2314...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.2574...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.2325...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.2368...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.2411...  0.3098 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.2597...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.2508...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.2317...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.2184...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.2533...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.2636...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.2420...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.2571...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.2292...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.2584...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.2355...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.2681...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.2447...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.1930...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.2084...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.2543...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.2518...  0.3105 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.2579...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.2253...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.2195...  0.3108 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.2445...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.2395...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.2281...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.2403...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.2155...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1967...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.1999...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.2308...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.2112...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.2740...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.2235...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.2101...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.2529...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.2199...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.2254...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.2265...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.2381...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.2594...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.2045...  0.3087 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.2705...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.2479...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.2509...  0.3098 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.2294...  0.3087 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.2404...  0.3106 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.2488...  0.3109 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.2203...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.2144...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.2728...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.2511...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.2803...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.2583...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.2461...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.2398...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.2488...  0.3114 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.2604...  0.3104 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.2184...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.2446...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.2318...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.2872...  0.3085 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.2554...  0.3081 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.2637...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.2068...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.2445...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.2555...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.2396...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.2327...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.1989...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.2407...  0.3107 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.1951...  0.3106 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.2288...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.2099...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.2286...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.2099...  0.3087 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.2411...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.2186...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.2179...  0.3113 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.2089...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.2348...  0.3110 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.2130...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.2275...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.2193...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.2136...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.2262...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.2487...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.2421...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.1974...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.2203...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.2122...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.2304...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.2355...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.2384...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.2275...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.2312...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.2256...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.2303...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.2353...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.2212...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.2557...  0.3105 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.2103...  0.3104 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.2425...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.2263...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.2264...  0.3188 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.2031...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.1970...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.2380...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.2449...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.2315...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.2248...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.2396...  0.3083 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.2090...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.1945...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.2296...  0.3135 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.2163...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.1904...  0.3087 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.2396...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.2387...  0.3098 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.2117...  0.3098 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.1856...  0.3098 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.1796...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.2150...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.2530...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.2304...  0.3086 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.2352...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.2349...  0.3104 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.2693...  0.3108 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.2447...  0.3107 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.2354...  0.3091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.2423...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.2774...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.2380...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.2339...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.2656...  0.3106 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.2160...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.2563...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.2562...  0.3118 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.2633...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.2645...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.2264...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.2190...  0.3104 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.2147...  0.3089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.2461...  0.3117 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.2251...  0.3107 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.2227...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.2286...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.2368...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.2243...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.2072...  0.3099 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.2529...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.2579...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.2310...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.2261...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.2304...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.2300...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.2388...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.2752...  0.3095 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.2883...  0.3088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.2596...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.2378...  0.3108 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.2275...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.2349...  0.3094 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.2622...  0.3086 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.2360...  0.3086 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.2464...  0.3084 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.2118...  0.3101 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.2311...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.2718...  0.3098 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.2108...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.2083...  0.3110 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.2207...  0.3109 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.2343...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.2257...  0.3092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.2216...  0.3102 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.2209...  0.3106 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.2167...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.2636...  0.3093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.2296...  0.3155 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.2275...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.2300...  0.3107 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.2069...  0.3100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.2154...  0.3097 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.2297...  0.3098 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.2186...  0.3087 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.1902...  0.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.2343...  0.3096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.2209...  0.3103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2274...  0.3119 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512.ckpt\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512.ckpt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farce and his book, who had a conversation on the sorrest things that was\n",
      "that, but the moment he saw that she was too, because he would be the footman or the\n",
      "marshal, and at the more of the sumjerings to him, well and sense that had\n",
      "not conceared that anyounce that his wife's all should be over the same,\n",
      "but whispress his father-hearted heart to any and or humiliated the memory, and\n",
      "she was almost of anything, he had not telegram that there was all of in\n",
      "the first time human, and that soothe that it was that he had been surred to him. The\n",
      "prestry was, to have been set to the same time, had transag about his high\n",
      "strick a smile, she felt, as a little was his brother and, as though\n",
      "they had stirned, and therefwen her own weeking of her sungrished, and he was\n",
      "singling. She called her hands, and at the string of the same, and he\n",
      "could not be so good he was closed to the prince, and she felt the\n",
      "present of the same intension of the senselees and home and so\n",
      "things, and which she saw it and with the sort of horrie and walk, and\n",
      "straight on her stell, and was so so that he came in to the creacuse of\n",
      "her face of this. But her carriage they were serious and wattened to\n",
      "the support of the few starent of her hands. How a single hat been a singer\n",
      "and all the servant of a chair in the shade that impossible for her, and\n",
      "a little crowd with his face waisted.\n",
      "\n",
      "\"That's not at any one of the same is to strange,\" said Stepan Arkadyevitch.\n",
      "\"What do you won't be anything,\" he said to him,\n",
      "\"and what is that it was! And if they, are there.\"\n",
      "\n",
      "\"That's the society! I denected to be all to tran. It's not a great\n",
      "part of the thing. They were not a seet of tears in their men. He's no\n",
      "least.\" But the carriage and the trouble when he had been a gentlemen to\n",
      "him, holding him about the morning that she came out of the carriage to\n",
      "the part of them, and at all these principlits, which would need him, to be\n",
      "there to himself why she said:\n",
      "\n",
      "\"I am not so strange. I have noisely anow that hast of the fairt o\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fardd,\n",
      ", asthar te tor ondis he his onet hhe weto te se this ote whe he sis al the\n",
      "tint to thir hot tires int hire thons ontind tou thas osist and as ho thes orotin hhe wos toritithe and ansing ansares he sorad, hor ate at ant in ther hirito her antese hores itherinngore wos timer th are winthes ton wet an to alithe himeris oon on the won an hot ha and ote whed whe ase on he thet hos ote ther sotinn he hh mot hed ant he sere sotor ale wand anth womhor, astithere asd hesis and he tisth thed he thon hedsand ton har hin ha seer ins inter hins wos oot an hered wint he to on toungesthe th an ore tot on wot has ated hon thas hor tor hin tar th and an himere her os he werind he wint heran ot heded hars ha whos ot ane hintetas whond\n",
      "af ha thise he soreserann of times het an the wirth ser arid tot ou at hher asis of wote woto te te that he thes ane hed ans orethe he this ood this hass ifer in ser on an sare hir has hin ond that tot hor athe anto ad ale sher sot he so ont tor whe atinghe se hont at \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farned. Him, som she he deald\n",
      "to doring to she\n",
      "partite,\n",
      "and that the\n",
      "crimat that has well han stide, had she sead\n",
      "ander oter his\n",
      "sond ablut than stean that,\n",
      "and his talkned\n",
      "whord an the wan tere the she worded\n",
      "shis what that that her\n",
      "and she sand the chonterent that\n",
      "he hat to sore tores\n",
      "of it has that the soud as the her head, so thas header, and wist her thank ow and while wit him, buntion to har a to stre hord wall sered whe shers and and san with shor intinget at to the houd her sont ane had andes had seen the\n",
      "mont att them, wheth alred\n",
      "had the hers him. Ste he he whith at in and\n",
      "the\n",
      "prousing.\"\n",
      "\n",
      "The reant, that if her had buct on said to her has, the seid homen thene the pillating of inte of an of the ruther of the camenten the prised her\n",
      "his wall at the ward had, and ald and sterent of tha mele, when\n",
      "ha wing her hede some in hit ane a to doring this thould and\n",
      "and sho hus sade out were a that in the samestion of his his sand the\n",
      "maring\n",
      "his stace in the serong the her hed thas to a seid\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farcties what to she was a lat of the courter that strood tractiat, her a childen and she was ataled and and her\n",
      "had at the possan of with with the some of to she wasted to have a lowgle him,er and saw, too, while he had to him, he susted to all the propented and told the\n",
      "seaming of which at the prosint him that\n",
      "say hears the peacents who wad into the passiance. And he saw as he had, and so would be a compleant was and with sont and strange a so midation.\n",
      "\n",
      "She wis that to she was not in herse a porten think was that his his was offering once his had heard and talking at the plate of a solt wholes had not seeth of the sarmina, whise he curce someter, which the samore saw at his fince at the plesser was though, a say of seemed and the saming the thish, but the seamed heard to alone, hapring, he would say, say a gond and with stright, but the peesance of his\n",
      "his for he wilied to thick the stiliat from his\n",
      "comprite as the tell than the propon was her happented, and the truse along a the shile \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
