{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy in TensorFlow\n",
    "* To create a cross entropy function in TensorFlow, you'll need to use two new functions:\n",
    "```python\n",
    "tf.reduce_sum()\n",
    "tf.log()\n",
    "```\n",
    "* **Reduce Sum**\n",
    "```python\n",
    "x = tf.reduce_sum( [1, 2, 3, 4, 5] ) # 15\n",
    "```\n",
    "  * The ```tf.reduce_sum()``` function takes an array of numbers ans sum them together.\n",
    "* **Natural Log**\n",
    "```python\n",
    "x = tf.log(100) # 4.60517\n",
    "```\n",
    "  * This function does exactly what you would expect it to do. ```tf.log()``` takes the natural log of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "# Quiz: Print the cross entropy using softmax_data and one_hot_encod_label.\n",
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "product = tf.multiply(one_hot, tf.log(softmax))\n",
    "cross_entropy = -tf.reduce_sum( product )\n",
    "\n",
    "with tf.Session() as session:\n",
    "    ce = session.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})\n",
    "    print(ce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
