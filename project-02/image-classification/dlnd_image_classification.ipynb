{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 1:\n",
      "Image - Min Value: 2 Max Value: 247\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmKIHgML+H2mNv660C5tZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb772ef3a20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 1\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    normalized = x * (1.0/255.0)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # previous implementation\n",
    "    #number_of_labels = 10\n",
    "    #data_length = len(x)\n",
    "    #one_hot_encoded = np.zeros((data_length, number_of_labels), dtype=np.int32)\n",
    "    \n",
    "    #for i in range(data_length):\n",
    "    #    index = x[i]\n",
    "    #    one_hot_encoded[i][index] = 1\n",
    "    #return one_hot_encoded\n",
    "    \n",
    "    # reviewer recomended\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(10))\n",
    "    return label_binarizer.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # previous implementation\n",
    "    #shape_to_tf = [None]\n",
    "    #shape_to_tf.extend(list(image_shape))\n",
    "    #return tf.placeholder(tf.float32, shape=shape_to_tf, name='x')\n",
    "    \n",
    "    # reviewer recomended\n",
    "    return tf.placeholder(tf.float32, shape=[None, *image_shape], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape_to_tf = [None, n_classes]\n",
    "    return tf.placeholder(tf.float32, shape=shape_to_tf, name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # get input image depth\n",
    "    input_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # create weight and bias\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=[*conv_ksize, input_shape[-1], conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # apply convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1, *conv_strides, 1], padding='SAME')\n",
    "    \n",
    "    # add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    \n",
    "    # add nonlinear activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    # apply max_pooling\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_shape = x_tensor.get_shape().as_list()\n",
    "    flatten_size = input_shape[1] * input_shape[2] * input_shape[3]\n",
    "    \n",
    "    input_flat = tf.reshape(x_tensor, [-1, flatten_size])\n",
    "    return input_flat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    input_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # create weight and bias\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[input_shape[-1], num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # compute fully connected layer\n",
    "    fc = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    input_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # create weight and bias\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[input_shape[-1], num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # compute output layer\n",
    "    output = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # prepare variables\n",
    "    filter1_ksize = (5,5)\n",
    "    filter1_depth = 16\n",
    "    filter2_ksize = (5,5)\n",
    "    filter2_depth = 32\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    fc_size = 128\n",
    "    number_of_class = 10\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_layer = conv2d_maxpool(x, filter1_depth, filter1_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(conv_layer, filter2_depth, filter2_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_layer = flatten(conv_layer)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    # reviewer recomemded: add dropout to fully connected layer\n",
    "    fc = fully_conn(flat_layer, fc_size)\n",
    "    fc = tf.nn.dropout(fc, keep_prob=keep_prob)\n",
    "    fc = fully_conn(fc, fc_size)\n",
    "    #fc = tf.nn.dropout(fc, keep_prob=keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logit = output(fc, number_of_class)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return logit\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    acc = session.run(accuracy, feed_dict={x: valid_features, y:valid_labels, keep_prob:1.0})\n",
    "    print(\"loss=\", \"{:.5f}\".format(loss), \"accuracy=\", \"{:.5f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss= 2.04174 accuracy= 0.27340\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss= 1.80928 accuracy= 0.39360\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss= 1.64700 accuracy= 0.42740\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss= 1.49555 accuracy= 0.45380\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss= 1.34517 accuracy= 0.47020\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss= 1.20933 accuracy= 0.47520\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss= 1.08052 accuracy= 0.49640\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss= 0.96316 accuracy= 0.50000\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss= 0.81609 accuracy= 0.51780\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss= 0.77405 accuracy= 0.51860\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss= 0.72842 accuracy= 0.52360\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss= 0.63766 accuracy= 0.53700\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss= 0.60605 accuracy= 0.52500\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss= 0.53118 accuracy= 0.53900\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss= 0.44816 accuracy= 0.54020\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss= 0.36236 accuracy= 0.55660\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss= 0.33228 accuracy= 0.56440\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss= 0.31861 accuracy= 0.57840\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss= 0.25899 accuracy= 0.58260\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss= 0.22695 accuracy= 0.58120\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss= 0.24174 accuracy= 0.57880\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss= 0.25628 accuracy= 0.56600\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss= 0.18406 accuracy= 0.57520\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss= 0.18417 accuracy= 0.56200\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss= 0.19164 accuracy= 0.55640\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss= 0.14441 accuracy= 0.58580\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss= 0.14078 accuracy= 0.56520\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss= 0.14617 accuracy= 0.55100\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss= 0.11808 accuracy= 0.57820\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss= 0.10700 accuracy= 0.56940\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss= 0.09893 accuracy= 0.56760\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss= 0.10467 accuracy= 0.57060\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss= 0.08211 accuracy= 0.57980\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss= 0.05400 accuracy= 0.59380\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss= 0.05173 accuracy= 0.58500\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss= 0.04862 accuracy= 0.59040\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss= 0.04392 accuracy= 0.59220\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss= 0.04323 accuracy= 0.58200\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss= 0.03612 accuracy= 0.58720\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss= 0.03344 accuracy= 0.59840\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss= 0.01831 accuracy= 0.60480\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss= 0.02551 accuracy= 0.58980\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss= 0.02948 accuracy= 0.59300\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss= 0.01883 accuracy= 0.59540\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss= 0.01230 accuracy= 0.59780\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss= 0.01919 accuracy= 0.57880\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss= 0.02585 accuracy= 0.58020\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss= 0.01476 accuracy= 0.57740\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss= 0.01742 accuracy= 0.58360\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss= 0.01184 accuracy= 0.58860\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss= 0.00905 accuracy= 0.58460\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss= 0.00678 accuracy= 0.58700\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss= 0.00792 accuracy= 0.58700\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss= 0.00832 accuracy= 0.58860\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss= 0.01214 accuracy= 0.57540\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss= 0.01147 accuracy= 0.57800\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss= 0.01456 accuracy= 0.57760\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss= 0.00572 accuracy= 0.58340\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss= 0.00420 accuracy= 0.58940\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss= 0.00453 accuracy= 0.58720\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss= 0.00419 accuracy= 0.57660\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss= 0.00393 accuracy= 0.57920\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss= 0.00273 accuracy= 0.58280\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss= 0.00594 accuracy= 0.57040\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss= 0.00512 accuracy= 0.57540\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss= 0.00630 accuracy= 0.56860\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss= 0.00235 accuracy= 0.58240\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss= 0.00289 accuracy= 0.57840\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss= 0.00561 accuracy= 0.56720\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss= 0.00337 accuracy= 0.56900\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss= 0.00223 accuracy= 0.58780\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss= 0.00252 accuracy= 0.57820\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss= 0.00237 accuracy= 0.57500\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss= 0.00169 accuracy= 0.57360\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss= 0.00105 accuracy= 0.58060\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss= 0.00286 accuracy= 0.57380\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss= 0.00797 accuracy= 0.56700\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss= 0.00187 accuracy= 0.58220\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss= 0.00158 accuracy= 0.58040\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss= 0.00114 accuracy= 0.58520\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss= 0.00124 accuracy= 0.58380\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss= 0.00146 accuracy= 0.58080\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss= 0.00104 accuracy= 0.58480\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss= 0.00190 accuracy= 0.57900\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss= 0.00109 accuracy= 0.57840\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss= 0.00034 accuracy= 0.58420\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss= 0.00090 accuracy= 0.58020\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss= 0.00069 accuracy= 0.58600\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss= 0.00043 accuracy= 0.58580\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss= 0.00108 accuracy= 0.58840\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss= 0.00083 accuracy= 0.58200\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss= 0.00060 accuracy= 0.58880\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss= 0.00046 accuracy= 0.57940\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss= 0.00042 accuracy= 0.58500\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss= 0.00042 accuracy= 0.58540\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss= 0.00141 accuracy= 0.58240\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss= 0.00070 accuracy= 0.57560\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss= 0.00050 accuracy= 0.58760\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss= 0.00024 accuracy= 0.58120\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss= 0.00072 accuracy= 0.56200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss= 2.07394 accuracy= 0.35400\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss= 1.76347 accuracy= 0.39500\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss= 1.50559 accuracy= 0.41660\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss= 1.61080 accuracy= 0.42420\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss= 1.63056 accuracy= 0.44080\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss= 1.67844 accuracy= 0.46540\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss= 1.43360 accuracy= 0.49260\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss= 1.12117 accuracy= 0.49760\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss= 1.36138 accuracy= 0.50040\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss= 1.37564 accuracy= 0.51860\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss= 1.40815 accuracy= 0.51580\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss= 1.20542 accuracy= 0.51760\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss= 0.97600 accuracy= 0.51200\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss= 1.13604 accuracy= 0.54740\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss= 1.18111 accuracy= 0.52720\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss= 1.21645 accuracy= 0.55120\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss= 1.00801 accuracy= 0.56240\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss= 0.83116 accuracy= 0.54480\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss= 0.99397 accuracy= 0.57680\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss= 1.01861 accuracy= 0.56240\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss= 0.98539 accuracy= 0.57720\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss= 0.83681 accuracy= 0.58120\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss= 0.71096 accuracy= 0.57240\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss= 0.91030 accuracy= 0.59180\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss= 0.90131 accuracy= 0.58880\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss= 0.89772 accuracy= 0.58580\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss= 0.72128 accuracy= 0.60240\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss= 0.66269 accuracy= 0.59800\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss= 0.82885 accuracy= 0.60900\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss= 0.73770 accuracy= 0.62380\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss= 0.76422 accuracy= 0.61600\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss= 0.64220 accuracy= 0.60660\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss= 0.64245 accuracy= 0.61140\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss= 0.72359 accuracy= 0.62060\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss= 0.63176 accuracy= 0.61580\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss= 0.73347 accuracy= 0.62160\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss= 0.55580 accuracy= 0.61060\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss= 0.49415 accuracy= 0.63740\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss= 0.65875 accuracy= 0.62740\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss= 0.58611 accuracy= 0.64040\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss= 0.62795 accuracy= 0.63120\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss= 0.43831 accuracy= 0.64220\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss= 0.44182 accuracy= 0.64400\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss= 0.63150 accuracy= 0.63220\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss= 0.54682 accuracy= 0.64580\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss= 0.55256 accuracy= 0.63960\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss= 0.41069 accuracy= 0.64720\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss= 0.41256 accuracy= 0.64460\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss= 0.56778 accuracy= 0.64460\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss= 0.44982 accuracy= 0.65320\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss= 0.49103 accuracy= 0.64780\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss= 0.38861 accuracy= 0.65220\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss= 0.39491 accuracy= 0.65840\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss= 0.51479 accuracy= 0.64640\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss= 0.42291 accuracy= 0.65180\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss= 0.46080 accuracy= 0.66360\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss= 0.35375 accuracy= 0.64820\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss= 0.37456 accuracy= 0.66420\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss= 0.42600 accuracy= 0.65700\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss= 0.38174 accuracy= 0.66560\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss= 0.40761 accuracy= 0.65780\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss= 0.33153 accuracy= 0.65560\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss= 0.32129 accuracy= 0.65920\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss= 0.38453 accuracy= 0.67140\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss= 0.36137 accuracy= 0.66340\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss= 0.36604 accuracy= 0.66300\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss= 0.30690 accuracy= 0.66260\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss= 0.27668 accuracy= 0.66020\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss= 0.34516 accuracy= 0.66960\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss= 0.33834 accuracy= 0.67560\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss= 0.35019 accuracy= 0.67480\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss= 0.32986 accuracy= 0.67520\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss= 0.26608 accuracy= 0.67760\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss= 0.29895 accuracy= 0.67320\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss= 0.25425 accuracy= 0.67180\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss= 0.31209 accuracy= 0.67600\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss= 0.27868 accuracy= 0.66840\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss= 0.24880 accuracy= 0.66920\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss= 0.31478 accuracy= 0.64880\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss= 0.23851 accuracy= 0.67360\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss= 0.33355 accuracy= 0.66620\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss= 0.24501 accuracy= 0.67980\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss= 0.18581 accuracy= 0.68040\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss= 0.28483 accuracy= 0.65860\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss= 0.18115 accuracy= 0.67540\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss= 0.30232 accuracy= 0.66860\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss= 0.22142 accuracy= 0.68440\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss= 0.18060 accuracy= 0.67400\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss= 0.30528 accuracy= 0.65000\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss= 0.17445 accuracy= 0.67540\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss= 0.26698 accuracy= 0.67360\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss= 0.22884 accuracy= 0.66420\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss= 0.16672 accuracy= 0.66860\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss= 0.21991 accuracy= 0.68200\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss= 0.17567 accuracy= 0.67500\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss= 0.27948 accuracy= 0.66440\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss= 0.22012 accuracy= 0.68160\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss= 0.15916 accuracy= 0.67360\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss= 0.22712 accuracy= 0.67060\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss= 0.16351 accuracy= 0.67960\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss= 0.26025 accuracy= 0.66720\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss= 0.20595 accuracy= 0.65600\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss= 0.18712 accuracy= 0.65880\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss= 0.19739 accuracy= 0.66420\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss= 0.15789 accuracy= 0.68140\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss= 0.19547 accuracy= 0.68180\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss= 0.16286 accuracy= 0.68560\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss= 0.13325 accuracy= 0.67120\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss= 0.19938 accuracy= 0.67640\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss= 0.13591 accuracy= 0.68160\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss= 0.17063 accuracy= 0.68260\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss= 0.16827 accuracy= 0.67560\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss= 0.12573 accuracy= 0.68200\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss= 0.19169 accuracy= 0.65840\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss= 0.10895 accuracy= 0.68040\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss= 0.16870 accuracy= 0.68200\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss= 0.13303 accuracy= 0.67220\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss= 0.13496 accuracy= 0.67220\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss= 0.18583 accuracy= 0.67420\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss= 0.12733 accuracy= 0.67800\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss= 0.18948 accuracy= 0.67700\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss= 0.11010 accuracy= 0.67460\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss= 0.10292 accuracy= 0.66780\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss= 0.13134 accuracy= 0.67680\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss= 0.11310 accuracy= 0.67420\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss= 0.15311 accuracy= 0.68020\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss= 0.10718 accuracy= 0.68220\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss= 0.10860 accuracy= 0.65720\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss= 0.11883 accuracy= 0.67480\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss= 0.12909 accuracy= 0.66360\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss= 0.12342 accuracy= 0.68820\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss= 0.09252 accuracy= 0.66560\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss= 0.10582 accuracy= 0.66220\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss= 0.11937 accuracy= 0.68080\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss= 0.08877 accuracy= 0.67420\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss= 0.13616 accuracy= 0.66780\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss= 0.09052 accuracy= 0.66360\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss= 0.09029 accuracy= 0.68480\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss= 0.09073 accuracy= 0.68000\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss= 0.08875 accuracy= 0.68340\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss= 0.13066 accuracy= 0.67580\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss= 0.10257 accuracy= 0.65220\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss= 0.09171 accuracy= 0.68420\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss= 0.09796 accuracy= 0.67920\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss= 0.08799 accuracy= 0.67740\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss= 0.12245 accuracy= 0.67580\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss= 0.08129 accuracy= 0.65160\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss= 0.11700 accuracy= 0.66660\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss= 0.09891 accuracy= 0.68540\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss= 0.07266 accuracy= 0.68800\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss= 0.12643 accuracy= 0.67140\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss= 0.06558 accuracy= 0.65500\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss= 0.09185 accuracy= 0.66820\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss= 0.08644 accuracy= 0.68040\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss= 0.07236 accuracy= 0.68240\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss= 0.13353 accuracy= 0.67400\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss= 0.07883 accuracy= 0.63880\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss= 0.06497 accuracy= 0.67960\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss= 0.06546 accuracy= 0.67960\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss= 0.09458 accuracy= 0.67860\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss= 0.12889 accuracy= 0.66300\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss= 0.06484 accuracy= 0.64880\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss= 0.07297 accuracy= 0.67640\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss= 0.07065 accuracy= 0.67240\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss= 0.05395 accuracy= 0.68660\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss= 0.09898 accuracy= 0.65900\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss= 0.05846 accuracy= 0.64340\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss= 0.07183 accuracy= 0.67820\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss= 0.06468 accuracy= 0.68020\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss= 0.06131 accuracy= 0.68220\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss= 0.07916 accuracy= 0.68460\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss= 0.04705 accuracy= 0.65740\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss= 0.04852 accuracy= 0.68000\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss= 0.08168 accuracy= 0.68260\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss= 0.04888 accuracy= 0.67340\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss= 0.07557 accuracy= 0.67700\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss= 0.05595 accuracy= 0.66280\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss= 0.04003 accuracy= 0.67960\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss= 0.06860 accuracy= 0.67980\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss= 0.05015 accuracy= 0.67740\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss= 0.06546 accuracy= 0.68000\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss= 0.05590 accuracy= 0.67600\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss= 0.03863 accuracy= 0.68360\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss= 0.07582 accuracy= 0.67080\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss= 0.04528 accuracy= 0.67980\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss= 0.06732 accuracy= 0.67720\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss= 0.06494 accuracy= 0.66820\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss= 0.03525 accuracy= 0.68060\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss= 0.04851 accuracy= 0.67560\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss= 0.04287 accuracy= 0.68500\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss= 0.05447 accuracy= 0.67160\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss= 0.04782 accuracy= 0.67900\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss= 0.03822 accuracy= 0.68220\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss= 0.05004 accuracy= 0.67800\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss= 0.03798 accuracy= 0.68440\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss= 0.04725 accuracy= 0.67720\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss= 0.03752 accuracy= 0.68560\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss= 0.03347 accuracy= 0.68440\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss= 0.05460 accuracy= 0.68100\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss= 0.03720 accuracy= 0.67560\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss= 0.05293 accuracy= 0.67520\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss= 0.04540 accuracy= 0.68700\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss= 0.04209 accuracy= 0.68000\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss= 0.03397 accuracy= 0.67660\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss= 0.02833 accuracy= 0.67540\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss= 0.04574 accuracy= 0.66580\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss= 0.03466 accuracy= 0.68180\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss= 0.02722 accuracy= 0.68180\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss= 0.04699 accuracy= 0.67240\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss= 0.04948 accuracy= 0.67600\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss= 0.04292 accuracy= 0.67480\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss= 0.05009 accuracy= 0.68120\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss= 0.02264 accuracy= 0.68620\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss= 0.04640 accuracy= 0.66680\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss= 0.04165 accuracy= 0.67720\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss= 0.04127 accuracy= 0.66440\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss= 0.03002 accuracy= 0.68700\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss= 0.01850 accuracy= 0.69040\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss= 0.03713 accuracy= 0.67360\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss= 0.03169 accuracy= 0.67600\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss= 0.03741 accuracy= 0.67100\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss= 0.02939 accuracy= 0.68000\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss= 0.03111 accuracy= 0.68340\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss= 0.03546 accuracy= 0.67100\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss= 0.03617 accuracy= 0.67620\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss= 0.04888 accuracy= 0.65380\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss= 0.02258 accuracy= 0.67420\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss= 0.02753 accuracy= 0.66940\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss= 0.05483 accuracy= 0.67100\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss= 0.03180 accuracy= 0.68080\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss= 0.04326 accuracy= 0.65600\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss= 0.02931 accuracy= 0.66420\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss= 0.01925 accuracy= 0.68180\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss= 0.04056 accuracy= 0.67680\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss= 0.03176 accuracy= 0.68100\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss= 0.04012 accuracy= 0.65340\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss= 0.02673 accuracy= 0.67760\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss= 0.01674 accuracy= 0.68400\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss= 0.03137 accuracy= 0.66420\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss= 0.02904 accuracy= 0.67920\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss= 0.03815 accuracy= 0.66980\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss= 0.02904 accuracy= 0.67600\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss= 0.01548 accuracy= 0.67840\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss= 0.02838 accuracy= 0.65560\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss= 0.01661 accuracy= 0.68380\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss= 0.06693 accuracy= 0.64420\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss= 0.02968 accuracy= 0.66580\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss= 0.02051 accuracy= 0.68080\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss= 0.02914 accuracy= 0.67140\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss= 0.03355 accuracy= 0.67400\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss= 0.04365 accuracy= 0.66040\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss= 0.02482 accuracy= 0.67300\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss= 0.01283 accuracy= 0.68440\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss= 0.03918 accuracy= 0.66580\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss= 0.02104 accuracy= 0.67560\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss= 0.03883 accuracy= 0.65820\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss= 0.01984 accuracy= 0.67680\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss= 0.01600 accuracy= 0.67620\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss= 0.02506 accuracy= 0.66560\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss= 0.02660 accuracy= 0.67160\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss= 0.03509 accuracy= 0.66240\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss= 0.02405 accuracy= 0.67460\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss= 0.01232 accuracy= 0.68480\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss= 0.03158 accuracy= 0.67260\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss= 0.02567 accuracy= 0.66500\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss= 0.03211 accuracy= 0.66940\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss= 0.02701 accuracy= 0.66580\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss= 0.01999 accuracy= 0.68160\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss= 0.02668 accuracy= 0.67260\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss= 0.02601 accuracy= 0.66040\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss= 0.02736 accuracy= 0.67680\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss= 0.03811 accuracy= 0.66620\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss= 0.01439 accuracy= 0.67480\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss= 0.03101 accuracy= 0.66500\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss= 0.01475 accuracy= 0.66980\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss= 0.03260 accuracy= 0.67720\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss= 0.02422 accuracy= 0.67740\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss= 0.01185 accuracy= 0.67460\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss= 0.03349 accuracy= 0.66640\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss= 0.01457 accuracy= 0.66740\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss= 0.03558 accuracy= 0.67600\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss= 0.01697 accuracy= 0.66960\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss= 0.01693 accuracy= 0.67700\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss= 0.02505 accuracy= 0.65660\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss= 0.01539 accuracy= 0.67000\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss= 0.03652 accuracy= 0.67480\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss= 0.02021 accuracy= 0.66620\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss= 0.02013 accuracy= 0.66720\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss= 0.02461 accuracy= 0.66940\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss= 0.01471 accuracy= 0.67060\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss= 0.02854 accuracy= 0.68080\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss= 0.02031 accuracy= 0.67400\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss= 0.01956 accuracy= 0.66860\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss= 0.02353 accuracy= 0.64720\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss= 0.01935 accuracy= 0.65820\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss= 0.02468 accuracy= 0.67340\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss= 0.03064 accuracy= 0.66540\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss= 0.01515 accuracy= 0.67080\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss= 0.02103 accuracy= 0.66280\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss= 0.01062 accuracy= 0.65240\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss= 0.02950 accuracy= 0.67300\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss= 0.02713 accuracy= 0.66540\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss= 0.01077 accuracy= 0.67260\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss= 0.02268 accuracy= 0.67740\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss= 0.01701 accuracy= 0.67020\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss= 0.03654 accuracy= 0.67180\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss= 0.01739 accuracy= 0.66960\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss= 0.01197 accuracy= 0.67220\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss= 0.01864 accuracy= 0.67420\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss= 0.01689 accuracy= 0.66800\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss= 0.02906 accuracy= 0.67260\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss= 0.01393 accuracy= 0.67180\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss= 0.01201 accuracy= 0.68240\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss= 0.01672 accuracy= 0.66860\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss= 0.01206 accuracy= 0.66740\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss= 0.02419 accuracy= 0.67460\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss= 0.01755 accuracy= 0.66300\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss= 0.01112 accuracy= 0.68000\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss= 0.01866 accuracy= 0.66480\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss= 0.01410 accuracy= 0.66720\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss= 0.01975 accuracy= 0.67520\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss= 0.01093 accuracy= 0.66580\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss= 0.00896 accuracy= 0.67880\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss= 0.01177 accuracy= 0.66960\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss= 0.01333 accuracy= 0.67420\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss= 0.03644 accuracy= 0.68200\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss= 0.01613 accuracy= 0.66520\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss= 0.02274 accuracy= 0.67980\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss= 0.02239 accuracy= 0.67600\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss= 0.01310 accuracy= 0.67000\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss= 0.02855 accuracy= 0.66860\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss= 0.02162 accuracy= 0.66960\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss= 0.01596 accuracy= 0.68100\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss= 0.01137 accuracy= 0.66380\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss= 0.01114 accuracy= 0.67400\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss= 0.02390 accuracy= 0.67520\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss= 0.01575 accuracy= 0.66080\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss= 0.00696 accuracy= 0.67820\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss= 0.01536 accuracy= 0.66740\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss= 0.01557 accuracy= 0.67140\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss= 0.02256 accuracy= 0.67460\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss= 0.01176 accuracy= 0.66560\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss= 0.01291 accuracy= 0.68000\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss= 0.01315 accuracy= 0.67880\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss= 0.01111 accuracy= 0.67940\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss= 0.01773 accuracy= 0.67180\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss= 0.01437 accuracy= 0.66980\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss= 0.00825 accuracy= 0.67240\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss= 0.01311 accuracy= 0.66840\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss= 0.01152 accuracy= 0.68140\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss= 0.02393 accuracy= 0.67160\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss= 0.00981 accuracy= 0.66380\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss= 0.01162 accuracy= 0.67260\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss= 0.01365 accuracy= 0.66760\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss= 0.01408 accuracy= 0.67700\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss= 0.01766 accuracy= 0.67540\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss= 0.01035 accuracy= 0.66760\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss= 0.01620 accuracy= 0.68080\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss= 0.01549 accuracy= 0.67660\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss= 0.00886 accuracy= 0.67880\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss= 0.03075 accuracy= 0.67800\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss= 0.00641 accuracy= 0.67380\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss= 0.00868 accuracy= 0.67760\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss= 0.01501 accuracy= 0.66280\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss= 0.01855 accuracy= 0.66820\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss= 0.01445 accuracy= 0.67620\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss= 0.00788 accuracy= 0.67460\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss= 0.00999 accuracy= 0.67440\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss= 0.01021 accuracy= 0.66900\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss= 0.00893 accuracy= 0.67520\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss= 0.02720 accuracy= 0.67320\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss= 0.00957 accuracy= 0.67580\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss= 0.00607 accuracy= 0.67660\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss= 0.01494 accuracy= 0.66040\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss= 0.01036 accuracy= 0.67320\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss= 0.02839 accuracy= 0.67720\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss= 0.00798 accuracy= 0.67240\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss= 0.01018 accuracy= 0.67420\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss= 0.01421 accuracy= 0.67300\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss= 0.00675 accuracy= 0.67040\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss= 0.02469 accuracy= 0.67160\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss= 0.01177 accuracy= 0.66660\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss= 0.00586 accuracy= 0.67920\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss= 0.01020 accuracy= 0.66620\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss= 0.00606 accuracy= 0.66720\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss= 0.02216 accuracy= 0.68100\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss= 0.01276 accuracy= 0.66680\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss= 0.00559 accuracy= 0.67300\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss= 0.00949 accuracy= 0.67600\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss= 0.00535 accuracy= 0.67060\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss= 0.01359 accuracy= 0.67260\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss= 0.00665 accuracy= 0.67740\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss= 0.00636 accuracy= 0.67500\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss= 0.01015 accuracy= 0.66520\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss= 0.00576 accuracy= 0.65940\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss= 0.02042 accuracy= 0.68100\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss= 0.00592 accuracy= 0.67280\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss= 0.00337 accuracy= 0.67440\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss= 0.01021 accuracy= 0.67560\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss= 0.00914 accuracy= 0.65860\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss= 0.01764 accuracy= 0.67300\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss= 0.00750 accuracy= 0.67200\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss= 0.00322 accuracy= 0.68140\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss= 0.00642 accuracy= 0.67480\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss= 0.00776 accuracy= 0.66320\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss= 0.02549 accuracy= 0.66700\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss= 0.00463 accuracy= 0.66340\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss= 0.00442 accuracy= 0.67580\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss= 0.00987 accuracy= 0.66660\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss= 0.00943 accuracy= 0.66320\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss= 0.02579 accuracy= 0.67100\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss= 0.00845 accuracy= 0.65920\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss= 0.00530 accuracy= 0.66940\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss= 0.01176 accuracy= 0.66860\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss= 0.00854 accuracy= 0.66320\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss= 0.02567 accuracy= 0.67520\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss= 0.00350 accuracy= 0.67340\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss= 0.00360 accuracy= 0.67800\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss= 0.00626 accuracy= 0.66900\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss= 0.00582 accuracy= 0.66580\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss= 0.02317 accuracy= 0.66280\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss= 0.00707 accuracy= 0.66080\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss= 0.00256 accuracy= 0.67560\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss= 0.01225 accuracy= 0.67740\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss= 0.00642 accuracy= 0.66320\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss= 0.01945 accuracy= 0.66980\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss= 0.00439 accuracy= 0.66840\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss= 0.00525 accuracy= 0.67240\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss= 0.01304 accuracy= 0.66520\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss= 0.00551 accuracy= 0.66940\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss= 0.03717 accuracy= 0.67020\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss= 0.00824 accuracy= 0.66660\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss= 0.00448 accuracy= 0.66500\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss= 0.01001 accuracy= 0.67220\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss= 0.00629 accuracy= 0.67460\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss= 0.02948 accuracy= 0.66260\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss= 0.00477 accuracy= 0.66780\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss= 0.00206 accuracy= 0.67600\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss= 0.01067 accuracy= 0.67820\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss= 0.01075 accuracy= 0.65700\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss= 0.01535 accuracy= 0.65400\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss= 0.00573 accuracy= 0.66760\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss= 0.00479 accuracy= 0.68100\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss= 0.00818 accuracy= 0.67720\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss= 0.00751 accuracy= 0.67380\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss= 0.01368 accuracy= 0.65640\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss= 0.00547 accuracy= 0.66840\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss= 0.00487 accuracy= 0.67760\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss= 0.00810 accuracy= 0.67520\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss= 0.01103 accuracy= 0.67500\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss= 0.01545 accuracy= 0.66080\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss= 0.00389 accuracy= 0.66940\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss= 0.00494 accuracy= 0.67540\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss= 0.01117 accuracy= 0.67180\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss= 0.00673 accuracy= 0.66500\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss= 0.01498 accuracy= 0.66160\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss= 0.00567 accuracy= 0.67640\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss= 0.00545 accuracy= 0.68200\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss= 0.01113 accuracy= 0.67500\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss= 0.00607 accuracy= 0.67160\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss= 0.01291 accuracy= 0.66360\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss= 0.00633 accuracy= 0.67740\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss= 0.00366 accuracy= 0.68120\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss= 0.00430 accuracy= 0.68360\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss= 0.01207 accuracy= 0.67060\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss= 0.02560 accuracy= 0.65820\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss= 0.00949 accuracy= 0.67660\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss= 0.00473 accuracy= 0.68400\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss= 0.01118 accuracy= 0.67740\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss= 0.01560 accuracy= 0.67640\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss= 0.01388 accuracy= 0.66320\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss= 0.00642 accuracy= 0.68260\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss= 0.00332 accuracy= 0.68180\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss= 0.00637 accuracy= 0.66460\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss= 0.00655 accuracy= 0.67260\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss= 0.01621 accuracy= 0.66820\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss= 0.00832 accuracy= 0.68160\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss= 0.00507 accuracy= 0.67740\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss= 0.00647 accuracy= 0.66840\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss= 0.00691 accuracy= 0.67940\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss= 0.01142 accuracy= 0.66940\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss= 0.00511 accuracy= 0.67260\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss= 0.00265 accuracy= 0.68940\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss= 0.00743 accuracy= 0.67020\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss= 0.00487 accuracy= 0.67520\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss= 0.01644 accuracy= 0.66760\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss= 0.00583 accuracy= 0.67780\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss= 0.00310 accuracy= 0.67900\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss= 0.00876 accuracy= 0.67040\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss= 0.00874 accuracy= 0.67040\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss= 0.01632 accuracy= 0.66880\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss= 0.00847 accuracy= 0.67700\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss= 0.00353 accuracy= 0.68080\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss= 0.00661 accuracy= 0.66340\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss= 0.00704 accuracy= 0.67120\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss= 0.01650 accuracy= 0.66500\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss= 0.00404 accuracy= 0.67020\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss= 0.00634 accuracy= 0.68520\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss= 0.01084 accuracy= 0.67020\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss= 0.00455 accuracy= 0.68160\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.667578125\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb76bd1e5c0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
